{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d802a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import indic_transliteration\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate\n",
    "import ast\n",
    "from detectTransliteration import detect\n",
    "import logging\n",
    "import json\n",
    "import sqlite3\n",
    "import re\n",
    "import pandas as pd\n",
    "from sanskrit_parser import Parser\n",
    "from tabulate import tabulate\n",
    "import regex\n",
    "from itertools import groupby\n",
    "import unicodedata\n",
    "from typing import List, Dict, Any\n",
    "from sqlalchemy import create_engine, text, Column, String\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "## If heroku postgres\n",
    "#load_dotenv()\n",
    "#DATABASE_URL = os.environ['DATABASE_URL']\n",
    "\n",
    "#if local postgres\n",
    "\n",
    "#DATABASE_URL = \"postgresql+psycopg2://postgres:again@localhost:5432/sanskritmagicdb\"\n",
    "#DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "#DATABASE_URL = os.getenv(\"postgres://u5o7c326q19pvp:pdb08c4f74fd1c63df61a559fc3d7a261ac3c65a24df6cfd06fbb2ad511143f0d@c3cj4hehegopde.cluster-czrs8kj4isg7.us-east-1.rds.amazonaws.com:5432/d61ijmaljbh829\")\n",
    "\n",
    "#if DATABASE_URL.startswith(\"postgres://\"):\n",
    "    #DATABASE_URL = DATABASE_URL.replace(\"postgres://\", \"postgresql://\", 1)\n",
    "\n",
    "#if using SQLite\n",
    "\n",
    "DATABASE_URL = \"sqlite:///resources/merged_formdb.sqlite\"\n",
    "\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "##to get all the available schemes\n",
    "##indic_transliteration.sanscript.SCHEMES.keys()\n",
    "    \n",
    "def transliterateSLP1IAST(text):\n",
    "    return transliterate(text, sanscript.SLP1, sanscript.IAST)   \n",
    "\n",
    "def transliterateSLP1HK(text):\n",
    "    return transliterate(text, sanscript.SLP1, sanscript.HK)   \n",
    "\n",
    "def transliterateDEVSLP1(text):\n",
    "    return transliterate(text, sanscript.DEVANAGARI, sanscript.SLP1)\n",
    "        \n",
    "def anythingToSLP1(text):\n",
    "    detected_scheme_str = detect(text).upper()\n",
    "    detected_scheme = getattr(sanscript, detected_scheme_str)\n",
    "    return sanscript.transliterate(text, detected_scheme, sanscript.SLP1)\n",
    "def anythingToIAST(text):\n",
    "    detected_scheme_str = detect(text).upper()\n",
    "    detected_scheme = getattr(sanscript, detected_scheme_str)\n",
    "    return sanscript.transliterate(text, detected_scheme, sanscript.IAST)\n",
    "def anythingToHK(text):\n",
    "    detected_scheme_str = detect(text).upper()\n",
    "    detected_scheme = getattr(sanscript, detected_scheme_str)\n",
    "    return sanscript.transliterate(text, detected_scheme, sanscript.HK)\n",
    "\n",
    "def transliterateAnything(text, transliteration_scheme):\n",
    "    detected_scheme_str = detect(text).upper()\n",
    "    transliteration_scheme_str = transliteration_scheme.upper()\n",
    "    detected_scheme = getattr(sanscript, detected_scheme_str)\n",
    "    output_scheme = getattr(sanscript, transliteration_scheme_str)\n",
    "    return sanscript.transliterate(text, detected_scheme, output_scheme)\n",
    "\n",
    "##qui la lista degli encoding è lowercase\n",
    "\n",
    "\n",
    "parser = Parser(output_encoding='slp1')\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Session = sessionmaker(bind=engine)\n",
    "# Create a session\n",
    "session = Session()\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define the split_cache model\n",
    "class SplitCache(Base):\n",
    "    __tablename__ = 'split_cache'\n",
    "    \n",
    "    input = Column(String, primary_key=True)\n",
    "    splitted_text = Column(String)\n",
    "\n",
    "# Create the table if it doesn't exist\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "\n",
    "def sandhi_splitter(text_to_split):\n",
    "    \"\"\"\n",
    "    Splits the given text using a sandhi splitter parser.\n",
    "    Checks if the result is already cached before performing the split.\n",
    "\n",
    "    Parameters:\n",
    "    - text_to_split (str): The text to split.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of split parts of the text.\n",
    "    \"\"\"\n",
    "\n",
    "    #text_to_split = anythingToSLP1(text_to_split).strip()\n",
    "\n",
    "    # Check if the result is already cached\n",
    "    cached_result = session.query(SplitCache).filter_by(input=text_to_split).first()\n",
    "    \n",
    "    if cached_result:\n",
    "        # Retrieve and return the cached result if it exists\n",
    "        splitted_text = ast.literal_eval(cached_result.splitted_text)\n",
    "        print(f\"Retrieved from cache: {splitted_text}\")\n",
    "        return splitted_text\n",
    "\n",
    "    # If not cached, perform the split\n",
    "    try:\n",
    "        splits = parser.split(text_to_split, limit=1)\n",
    "\n",
    "        #if split is none, default to split by space\n",
    "        if splits is None:\n",
    "            return text_to_split.split()\n",
    "        for split in splits:\n",
    "            splitted_text = f'{split}'\n",
    "        splitted_text = ast.literal_eval(splitted_text)\n",
    "\n",
    "        print(f\"Splitted text: {splitted_text}\")\n",
    "\n",
    "        # Store the split result in cache as a list\n",
    "        new_cache_entry = SplitCache(input=text_to_split, splitted_text=str(splitted_text))\n",
    "        session.add(new_cache_entry)\n",
    "        session.commit()\n",
    "        print(f\"Added to cache: {splitted_text}\")\n",
    "\n",
    "        return splitted_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not split the line: {text_to_split}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return text_to_split.split()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stopwords = pd.read_csv('resources/stopwords.csv')\n",
    "\n",
    "stopwords_as_list = stopwords['stopword'].tolist()\n",
    "\n",
    "def remove_stopwords_list(text_list):\n",
    "    return [word for word in text_list if word not in stopwords_as_list]\n",
    "\n",
    "def remove_stopwords_string(text):\n",
    "    text = text.replace('.', '')  # Remove periods\n",
    "    text_list = text.split()  # Split the string into words\n",
    "    return ' '.join(word for word in text_list if word not in stopwords_as_list)\n",
    "\n",
    "\n",
    "with open('resources/MWKeysOnly.json', 'r', encoding='utf-8') as f:\n",
    "    mwdictionaryKeys = json.load(f)\n",
    "\n",
    "##given a name finds the root\n",
    "\n",
    "\n",
    "def SQLite_find_name(name):\n",
    "\n",
    "    outcome = []    \n",
    "\n",
    "    def query1(word):\n",
    "\n",
    "        session = Session()\n",
    "        try:\n",
    "            query_builder = text(\"SELECT * FROM lgtab2 WHERE key = :word\")\n",
    "            results = session.execute(query_builder, {'word': word}).fetchall()\n",
    "        except Exception as error:\n",
    "            print(\"Error while querying PostgreSQL:\", error)\n",
    "            results = []\n",
    "        finally:\n",
    "            session.close()\n",
    "        return results\n",
    "\n",
    "    results = query1(name)\n",
    "    \n",
    "    if not results:  # If query1 didn't find any results\n",
    "        if name[-1] == 'M':\n",
    "            name = name[:-1] + 'm'\n",
    "            results = query1(name)\n",
    "    \n",
    "    for inflected_form, type, root_form in results: \n",
    "        if not root_form:  # If root_form is None or empty\n",
    "            return  # End the function\n",
    "\n",
    "        def query2(root_form: str, type: str):\n",
    "\n",
    "            session = Session()\n",
    "            try:\n",
    "                query_builder2 = \"SELECT * FROM lgtab1 WHERE stem = :root_form and model = :type \"\n",
    "                results = session.execute(query_builder2, {'root_form': root_form, 'type': type}).fetchall()\n",
    "            except Exception as error:\n",
    "                print(\"Error while querying PostgreSQL:\", error)\n",
    "                results = []\n",
    "            finally:\n",
    "                session.close()\n",
    "            return results\n",
    "        \n",
    "        result = query2(root_form, type)\n",
    "        word_refs = re.findall(r\",([a-zA-Z]+)\",result[0][2])[0]\n",
    "        inflection_tuple = result[0][3]  # Get the first element of the first tuple\n",
    "        inflection_words = inflection_tuple.split(':') \n",
    "\n",
    "        ##transliterate back the result to IAST for readability\n",
    "        inflection_wordsIAST = [transliterateSLP1IAST(word) for word in inflection_words]\n",
    "        query_transliterateIAST = transliterateSLP1IAST(name)\n",
    "\n",
    "        ##make Inflection Table\n",
    "        indices = [i for i, x in enumerate(inflection_wordsIAST) if x == query_transliterateIAST]\n",
    "        rowtitles = [\"Nom\", \"Acc\", \"Inst\", \"Dat\", \"Abl\", \"Gen\", \"Loc\", \"Voc\"]\n",
    "        coltitles = [\"Sg\", \"Du\", \"Pl\"]\n",
    "\n",
    "        if indices:\n",
    "            row_col_names = [(rowtitles[i//3], coltitles[i%3]) for i in indices]\n",
    "        else: \n",
    "            row_col_names = None\n",
    "        outcome.append([word_refs, type, row_col_names, inflection_wordsIAST, name])\n",
    "\n",
    "    return outcome\n",
    "\n",
    "\n",
    "\n",
    "def SQLite_find_verb(verb):\n",
    "    \n",
    "    root_form = None\n",
    "\n",
    "    def query1(verb):\n",
    "\n",
    "        session = Session()\n",
    "        try:\n",
    "            query_builder = \"SELECT * FROM vlgtab2 WHERE key = :verb\"\n",
    "            results = session.execute(query_builder, {'verb': verb}).fetchall()\n",
    "        except Exception as error:\n",
    "            print(\"Error while querying PostgreSQL:\", error)\n",
    "            results = []\n",
    "        finally:\n",
    "            session.close()\n",
    "        return results\n",
    "\n",
    "    result = query1(verb)\n",
    "    \n",
    "    for inflected_form, type, root_form in result:\n",
    "\n",
    "        if not root_form:  # If root_form is None or empty\n",
    "            return  # End the function\n",
    "        type_var = type\n",
    "\n",
    "        def query2(root_form: str, type: str):\n",
    "\n",
    "            session = Session()\n",
    "            try:\n",
    "                query_builder2 = \"SELECT * FROM vlgtab1 WHERE stem = :root_form and model = :type\"\n",
    "                results = session.execute(query_builder2, {'root_form': root_form, 'type': type}).fetchall()\n",
    "            except Exception as error:\n",
    "                print(\"Error while querying PostgreSQL:\", error)\n",
    "                results = []\n",
    "            finally:\n",
    "                session.close()\n",
    "            return results\n",
    "        \n",
    "        result = query2(root_form, type)\n",
    "    \n",
    "    selected_tuple = None\n",
    "\n",
    "    # Iterate over the result list\n",
    "    for model, stem, refs, data in result:\n",
    "        if model == type_var:  # If the model matches type_var\n",
    "            ref_word = re.search(\",([a-zA-Z]+)\", refs).group(1)\n",
    "            if stem != ref_word:\n",
    "                stem= ref_word\n",
    "                #print(\"ref_word, stem\", ref_word, stem)\n",
    "                selected_tuple = (model, stem, refs, data)  # Get the entire tuple\n",
    "                break  # Exit the loop\n",
    "            selected_tuple = (model, stem, refs, data)  # Get the entire tuple\n",
    "            break  # Exit the loop\n",
    "\n",
    "    if selected_tuple is None:\n",
    "        #print(\"No matching model found in result\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    # Now you can use selected_tuple\n",
    "    inflection_tuple = selected_tuple[3]  # Get the 'data' element of the tuple\n",
    "    inflection_words = inflection_tuple.split(':') \n",
    "    \n",
    "    ##transliterate back the result to IAST for readability\n",
    "    \n",
    "    inflection_wordsIAST = [transliterateSLP1IAST(word) for word in inflection_words]\n",
    "    query_transliterateIAST = transliterateSLP1IAST(verb)\n",
    "    \n",
    "    ##make Inflection Table\n",
    "    \n",
    "    indices = [i for i, x in enumerate(inflection_wordsIAST) if x == query_transliterateIAST]\n",
    "\n",
    "    # Define row and column titles\n",
    "    rowtitles = [\"First\", \"Second\", \"Third\"]\n",
    "    coltitles = [\"Sg\", \"Du\", \"Pl\"]\n",
    "\n",
    "\n",
    "    if indices:\n",
    "        row_col_names = [(rowtitles[i//3], coltitles[i%3]) for i in indices]\n",
    "    else:\n",
    "        row_col_names = None\n",
    "        \n",
    "    return [[stem, type_var, row_col_names, inflection_wordsIAST, verb]]\n",
    "\n",
    "\n",
    "## also map to the type.\n",
    "# Read the Excel file into a DataFrame\n",
    "type_map = pd.read_excel('resources/type_map.xlsx')\n",
    "\n",
    "def root_any_word(word):\n",
    "\n",
    "\n",
    "    result_roots_name = SQLite_find_name(word)  \n",
    "    result_roots_verb = SQLite_find_verb(word) \n",
    "\n",
    "\n",
    "    if result_roots_name:\n",
    "        if result_roots_verb:\n",
    "            result_roots = result_roots_name + result_roots_verb\n",
    "        else:\n",
    "            result_roots = result_roots_name\n",
    "    else:\n",
    "        result_roots = result_roots_verb\n",
    "\n",
    "    if result_roots:\n",
    "        for i in range(len(result_roots)):\n",
    "            result = result_roots[i]\n",
    "            # Get the second member of the list\n",
    "            abbr = result[1]\n",
    "            # Find the matching value in the 'abbr' column\n",
    "            match = type_map[type_map['abbr'] == abbr]\n",
    "            \n",
    "            if not match.empty:\n",
    "                description = match['description'].values[0]\n",
    "                result[1] = description\n",
    "                result_roots[i] = result\n",
    "        return result_roots\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def root_any_list(text_list):\n",
    "    roots = []\n",
    "    for word in text_list:\n",
    "        root_found = root_any_word(word)\n",
    "        if root_found is not None:\n",
    "            roots.append(root_found)\n",
    "    \n",
    "    # Transliterate the first element of each tuple\n",
    "    for i in range(len(roots)):\n",
    "        roots[i] = (transliterateSLP1IAST(roots[i][0].replace('-', '')),) + roots[i][1:]\n",
    "    \n",
    "    return roots\n",
    "\n",
    "\n",
    "def dict_word_iterative(word):\n",
    "    sanskrit_end_letters = ['a', 'ā', 'i', 'ī', 'u', 'ū', 'e', 's']  # Add all Sanskrit end-of-word letters here\n",
    "    temp_word = word\n",
    "    removed_part = ''  # Part of the word that was removed\n",
    "    found = False  # Flag to indicate when a match has been found\n",
    "    if temp_word in mwdictionaryKeys:  \n",
    "            found_word = temp_word \n",
    "            found = True\n",
    "            return [found_word, removed_part]\n",
    "    while temp_word and not found: # Continue the loop until a match is found or temp_word is empty        \n",
    "        if temp_word[:-1] in mwdictionaryKeys:  \n",
    "            found_word = temp_word[:-1] \n",
    "            #print(temp_word[:-1] ) debug\n",
    "            found = True\n",
    "            return [found_word, removed_part]       \n",
    "        elif temp_word[-1].isalpha():  # If the last character is a letter\n",
    "            for letter in sanskrit_end_letters:\n",
    "                attempt = temp_word[:-1] + letter\n",
    "                #print(attempt) debug\n",
    "                if attempt in mwdictionaryKeys:\n",
    "                    found_word = attempt\n",
    "                    found = True  # Set the flag to True when a match is found\n",
    "                    break                                \n",
    "        removed_part = temp_word[-1] + removed_part  # Keep track of the removed part\n",
    "        temp_word = temp_word[:-1]  # Remove the last character, regardless of whether it's a letter or not\n",
    "    \n",
    "    return [found_word, removed_part] if found else None  # Return the match if found, else return None\n",
    "\n",
    "\n",
    "\n",
    "        ##if the dictionary approach fails, try the iterative approach:\n",
    "\n",
    "\n",
    "def root_compounds(word):\n",
    "\n",
    "    if word[0] == \"'\":\n",
    "        word = 'a' + word[1:]\n",
    "\n",
    "    first_root = dict_word_iterative(word)\n",
    "    #print(\"first_root\", first_root)\n",
    "    first_root_entry = root_any_word(first_root[0])\n",
    "    #print(\"first_root_entry\", first_root_entry)\n",
    "    \n",
    "    ## if it's a compound\n",
    "    if first_root is not None and first_root[1] is not None and len(first_root[1]) >= 4:\n",
    "        \n",
    "        ##remove the first root from the word\n",
    "        without_root = word.replace(first_root[0], '', 1)  # Only replace the first occurrence\n",
    "        \n",
    "        ##try the dictionary approach\n",
    "        second_root = root_any_word(without_root)\n",
    "        \n",
    "        ##if the dictionary approach fails, try the iterative approach:\n",
    "        if second_root == None:\n",
    "            second_root = dict_word_iterative(without_root)\n",
    "            #print(\"second_root\", second_root)\n",
    "            if second_root is None:\n",
    "                if first_root_entry is not None:\n",
    "                    return first_root_entry\n",
    "                else:\n",
    "                    return [first_root[0]]\n",
    "            if len(second_root[0]) < 2:\n",
    "                second_root = None\n",
    "            else:\n",
    "                second_root_try = root_any_word(second_root[0])\n",
    "                if second_root_try is not None:\n",
    "                    second_root = second_root_try\n",
    "                else: \n",
    "                    second_root = [second_root[0]]            \n",
    "            if second_root is not None:\n",
    "                if first_root_entry is not None:                    \n",
    "                    return first_root_entry + second_root\n",
    "                else:\n",
    "                    return [first_root[0]] + second_root\n",
    "            else:\n",
    "                if first_root_entry is not None:\n",
    "                    return first_root_entry\n",
    "                else:\n",
    "                    return [first_root[0]]\n",
    "        else:\n",
    "            if first_root_entry is not None:\n",
    "                return first_root_entry + second_root\n",
    "            else:\n",
    "                return [first_root[0]] + second_root\n",
    "            \n",
    "    ## if it's not a compound        \n",
    "    else:\n",
    "        if first_root_entry is not None:\n",
    "            return first_root_entry\n",
    "        else:\n",
    "            return [first_root[0]]\n",
    "            \n",
    "\n",
    "\n",
    "def root_compounds(word):\n",
    "    if word.startswith(\"'\"):\n",
    "        word = 'a' + word[1:]\n",
    "\n",
    "    # Base case: if word is empty, return empty list\n",
    "    if not word:\n",
    "        return []\n",
    "\n",
    "    roots = []\n",
    "\n",
    "    first_root = dict_word_iterative(word)\n",
    "    if not first_root or not first_root[0]:\n",
    "        # No root found\n",
    "        return []\n",
    "\n",
    "    first_root_entry = root_any_word(first_root[0])\n",
    "    if first_root_entry is not None:\n",
    "        if isinstance(first_root_entry, list):\n",
    "            roots.extend(first_root_entry)\n",
    "        else:\n",
    "            roots.append(first_root_entry)\n",
    "    else:\n",
    "        roots.append(first_root[0])\n",
    "\n",
    "    # Remove the first root from the word\n",
    "    without_root = word.replace(first_root[0], '', 1)\n",
    "\n",
    "    # If there's nothing left, return the roots found\n",
    "    if not without_root:\n",
    "        return roots\n",
    "\n",
    "    # Now, recursively process the remaining word\n",
    "    rest_entries = root_any_word(without_root)\n",
    "    if rest_entries is not None:\n",
    "        if isinstance(rest_entries, list):\n",
    "            roots.extend(rest_entries)\n",
    "        else:\n",
    "            roots.append(rest_entries)\n",
    "    else:\n",
    "        # Try dict_word_iterative and ensure result is longer than 2\n",
    "        rest_root = dict_word_iterative(without_root)\n",
    "        if rest_root is None or len(rest_root[0]) < 2:\n",
    "            # Cannot proceed further; return roots found so far\n",
    "            return roots\n",
    "        else:\n",
    "            # Recursively process the rest of the word\n",
    "            rest_roots = root_compounds(without_root)\n",
    "            roots.extend(rest_roots)\n",
    "\n",
    "    return roots\n",
    "\n",
    "\n",
    "def root_compounds2(word, entries=None, depth=0, max_depth=5):\n",
    "    \"\"\"\n",
    "    Recursively analyze compound words to find their constituents.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word to analyze\n",
    "        entries (list): Accumulated entries\n",
    "        depth (int): Current recursion depth\n",
    "        max_depth (int): Maximum recursion depth to prevent infinite loops\n",
    "        \n",
    "    Returns:\n",
    "        list: List of root constituents found in the word\n",
    "    \"\"\"\n",
    "    # Initialize entries list if None\n",
    "    if entries is None:\n",
    "        entries = []\n",
    "    \n",
    "    # Base cases\n",
    "    if depth >= max_depth or not word:\n",
    "        return entries\n",
    "    \n",
    "    # Handle special case with apostrophe\n",
    "    if word[0] == \"'\":\n",
    "        word = 'a' + word[1:]\n",
    "    \n",
    "    # Get first root using iterative approach\n",
    "    first_root = dict_word_iterative(word)\n",
    "    if first_root is None:\n",
    "        return entries\n",
    "    \n",
    "    # Get morphological analysis of the first root\n",
    "    first_root_entry = root_any_word(first_root[0])\n",
    "    \n",
    "    # Store first root entry or first root\n",
    "    if first_root_entry is not None:\n",
    "        entries.extend(first_root_entry)\n",
    "    else:\n",
    "        entries.append(first_root[0])\n",
    "    \n",
    "    # If the remainder is too short, return accumulated entries\n",
    "    if first_root[1] is None or len(first_root[1]) < 4:\n",
    "        return entries\n",
    "    \n",
    "    # Remove the first root from the word\n",
    "    remaining = word.replace(first_root[0], '', 1)\n",
    "    \n",
    "    # Try to analyze the remaining part\n",
    "    def analyze_remaining(remaining_word):\n",
    "        nonlocal entries\n",
    "        \n",
    "        # First try dictionary approach\n",
    "        remaining_root = root_any_word(remaining_word)\n",
    "        if remaining_root is not None:\n",
    "            entries.extend(remaining_root)\n",
    "            return True\n",
    "        \n",
    "        # If dictionary approach fails, try iterative approach\n",
    "        iterative_result = dict_word_iterative(remaining_word)\n",
    "        if iterative_result is None or len(iterative_result[0]) < 2:\n",
    "            return False\n",
    "            \n",
    "        # Try to get morphological analysis of the iterative result\n",
    "        remaining_root = root_any_word(iterative_result[0])\n",
    "        if remaining_root is not None:\n",
    "            entries.extend(remaining_root)\n",
    "            # Recursively analyze any remaining parts\n",
    "            if iterative_result[1]:\n",
    "                root_compounds2(iterative_result[1], entries, depth + 1, max_depth)\n",
    "            return True\n",
    "        \n",
    "        # If no morphological analysis, use the iterative result\n",
    "        entries.append(iterative_result[0])\n",
    "        # Recursively analyze any remaining parts\n",
    "        if iterative_result[1]:\n",
    "            root_compounds2(iterative_result[1], entries, depth + 1, max_depth)\n",
    "        return True\n",
    "    \n",
    "    # Analyze the remaining part\n",
    "    analyze_remaining(remaining)\n",
    "    return entries\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inflect(splitted_text):\n",
    "    roots = []\n",
    "    \n",
    "#    print(\"splitted\", splitted_text)\n",
    "    for word in splitted_text: \n",
    "        \n",
    "        rooted = root_any_word(word)\n",
    "        if rooted is not None:\n",
    "            for root in rooted:\n",
    "                roots.append(root)  \n",
    "        else:\n",
    "            print(\"here breaks\", word)\n",
    "            compound_try = root_compounds(word)\n",
    "            if compound_try is not None:\n",
    "                roots.extend(compound_try)  \n",
    "                #print(\"compound_try\", compound_try)\n",
    "            else:\n",
    "                roots.extend(word)  \n",
    "                \n",
    "    for i in range(len(roots)):\n",
    "        if isinstance(roots[i], list):\n",
    "            #print(\"debug\", roots)\n",
    "            #print(roots[i][0])\n",
    "            roots[i][0] = transliterateSLP1IAST(roots[i][0].replace('-', ''))\n",
    "        else:\n",
    "            #print(roots[i][0])\n",
    "            roots[i] = transliterateSLP1IAST(roots[i].replace('-', ''))           \n",
    "    #print(\"inflect roots\", roots)\n",
    "    return roots             \n",
    "\n",
    "## bug with process(\"nīlotpalapatrāyatākṣī\")    \n",
    "\n",
    "\n",
    "with open('resources/no_abbreviationMW.json') as f:\n",
    "    # Load JSON data from file\n",
    "    dictionary_json = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "def process(text):\n",
    "\n",
    "    if ' ' not in text:\n",
    "        #print(\"single_word\", text)\n",
    "        transliterated_text = anythingToSLP1(text)     \n",
    "        #print(\"transliterated_text\", transliterated_text)\n",
    "        text = regex.sub('[^\\p{L}\\']', '', transliterated_text)\n",
    "        ## here it should be transliterated to SLP1 before and added aH at the end instead of a\n",
    "        if text[0] == \"'\":\n",
    "            text = 'a' + text[1:]\n",
    "        if text[-1] == 'o':\n",
    "            text = text[:-1] + 'aH'\n",
    "        #print(\"text\", text)\n",
    "        if \"o'\" in text:\n",
    "            modified_text = re.sub(r\"o'\", \"aH a\", text)\n",
    "            #print(\"modified_text\", modified_text)\n",
    "            result = process(modified_text)\n",
    "            return result\n",
    "        result = root_any_word(text)\n",
    "        if result is None and text[-1] == 'M':\n",
    "            text = text[:-1] + 'm'\n",
    "            result = root_any_word(text)\n",
    "        if result is not None:\n",
    "            for res in result:\n",
    "                if isinstance(res, list):\n",
    "                    res[0] = transliterateSLP1IAST(res[0].replace('-', ''))\n",
    "                    #print(\"res\", res)\n",
    "            result_vocabulary = get_voc_entry(result)  \n",
    "            #print(\"result_vocabulary\", result_vocabulary)\n",
    "            return clean_results(result_vocabulary)\n",
    "        else:\n",
    "            query = [transliterateSLP1IAST(text)]\n",
    "            print(\"query\", query)\n",
    "            result_vocabulary = get_voc_entry(query)  \n",
    "            print(\"result_vocabulary\", result_vocabulary)\n",
    "            if result_vocabulary[0][0] != result_vocabulary[0][2][0]:\n",
    "                return clean_results(result_vocabulary)\n",
    "            \n",
    "    ## attempt to remove sandhi and tokenise in any case\n",
    "    print(\"pre_splitted_text\", text)\n",
    "    splitted_text = sandhi_splitter(text)    \n",
    "    inflections = inflect(splitted_text) \n",
    "    inflections_vocabulary = get_voc_entry(inflections)\n",
    "    inflections_vocabulary = [entry for entry in inflections_vocabulary if len(entry[0]) > 1]       \n",
    "\n",
    "    return clean_results(inflections_vocabulary)\n",
    "\n",
    "\n",
    "##process(\"dveṣānuviddhaścetanācetanasādhanādhīnastāpānubhava\")\n",
    "\n",
    "filtered_words = [\"ca\", \"na\", \"eva\", \"ni\", \"apya\", \"ava\"]\n",
    "\n",
    "\n",
    "def clean_results(list_of_entries):\n",
    "\n",
    "    i = 0\n",
    "    while i < len(list_of_entries) - 1:  # Subtract 1 to avoid index out of range error\n",
    "        # Check if the word is in filtered_words\n",
    "        if list_of_entries[i][0] in filtered_words:\n",
    "            while i < len(list_of_entries) - 1 and list_of_entries[i + 1][0] == list_of_entries[i][0]:\n",
    "                del list_of_entries[i + 1]\n",
    "        \n",
    "        # Check if the word is \"sam\"\n",
    "        if list_of_entries[i][0] == \"sam\":\n",
    "            j = i + 1\n",
    "            while j < len(list_of_entries) and (list_of_entries[j][0] == \"sa\" or list_of_entries[j][0] == \"sam\"):\n",
    "                j += 1\n",
    "            if j < len(list_of_entries):\n",
    "                voc_entry = get_voc_entry([\"sam\" + list_of_entries[j][0]])\n",
    "                print(\"voc_entry\", voc_entry)\n",
    "                if voc_entry[0][0] == voc_entry[0][2][0]:\n",
    "                    print(\"revised query\", [\"saM\" + list_of_entries[j][0]])\n",
    "                    voc_entry = process(\"saM\" + list_of_entries[j][0])\n",
    "                    print(\"revise_voc_entry\", voc_entry)\n",
    "                if voc_entry is not None:\n",
    "                    list_of_entries[i] = [item for sublist in voc_entry for item in sublist]\n",
    "                    del list_of_entries[i + 1:j + 1]\n",
    "        \n",
    "        # Check if the word is \"anu\"\n",
    "        if list_of_entries[i][0] == \"anu\":\n",
    "            j = i + 1\n",
    "            while j < len(list_of_entries) and (list_of_entries[j][0] == \"anu\"):\n",
    "                j += 1\n",
    "            if j < len(list_of_entries):\n",
    "                voc_entry = get_voc_entry([\"anu\" + list_of_entries[j][0]])\n",
    "                if voc_entry is not None:\n",
    "                    list_of_entries[i] = [item for sublist in voc_entry for item in sublist]\n",
    "                    del list_of_entries[i + 1:j + 1]\n",
    "        \n",
    "        # Check if the word is \"ava\"\n",
    "        if list_of_entries[i][0] == \"ava\":\n",
    "            j = i + 1\n",
    "            while j < len(list_of_entries) and (list_of_entries[j][0] == \"ava\"):\n",
    "                j += 1\n",
    "            if j < len(list_of_entries):\n",
    "                print(\"testing with:\", [\"ava\" + list_of_entries[j + 1][0]])\n",
    "                voc_entry = get_voc_entry([\"ava\" + list_of_entries[j + 1][0]])\n",
    "                if voc_entry is not None:\n",
    "                    list_of_entries[i] = [item for sublist in voc_entry for item in sublist]\n",
    "                    del list_of_entries[i + 1:j + 1]        \n",
    "        i += 1  \n",
    "    return list_of_entries\n",
    "\n",
    "def process_test(text, remove_stopwords = False, dictionary_entry = True, output_encoding = \"IAST\", entry_only = False):\n",
    "    \n",
    "    ## attempt to remove sandhi and tokenise in any case\n",
    "    splitted_text = sandhi_splitter(text)\n",
    "    \n",
    "    ## removes stopwords\n",
    "    if remove_stopwords == True: \n",
    "        splitted_text = remove_stopwords_list(splitted_text)\n",
    "        \n",
    "    \n",
    "    inflections = inflect(splitted_text) \n",
    "    \n",
    "    if dictionary_entry == True:\n",
    "        inflections = get_voc_entry(inflections)    \n",
    "\n",
    "    if entry_only == True:\n",
    "        entry_list = []\n",
    "        for entry in inflections:\n",
    "            entry_list.append(entry[0])\n",
    "        inflections = entry_list    \n",
    "\n",
    "    return clean_results(inflections)\n",
    "\n",
    "## hard mode testing:\n",
    "#process(\"dveṣānuviddhaścetanācetanasādhanādhīnastāpānubhava\")\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    remove_char_text = ''.join(c for c in text if c.isalpha() or c == \"'\" or c == ' ')\n",
    "\n",
    "    print(\"processing\", text)\n",
    "    ## attempt to remove sandhi and tokenise in any case\n",
    "    splitted_text = sandhi_splitter(remove_char_text)   \n",
    "\n",
    "    splitted_text = remove_stopwords_list(splitted_text)    \n",
    "    \n",
    "    inflections = inflect(splitted_text) \n",
    "    \n",
    "    entry_list = []\n",
    "    entry_list = [entry[0] for entry in inflections if len(entry[0]) > 1]\n",
    "    entry_list = [key for key, group in groupby(entry_list)]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(entry_list) - 1:\n",
    "        # Normalize words to form without diacritics\n",
    "        word1 = unicodedata.normalize('NFD', entry_list[i])\n",
    "        word2 = unicodedata.normalize('NFD', entry_list[i+1])\n",
    "\n",
    "        # Remove diacritics\n",
    "        word1 = ''.join(c for c in word1 if not unicodedata.combining(c))\n",
    "        word2 = ''.join(c for c in word2 if not unicodedata.combining(c))\n",
    "\n",
    "        if word1 in word2 or word2 in word1:\n",
    "            if len(entry_list[i]) > len(entry_list[i+1]):\n",
    "                entry_list.pop(i)\n",
    "            else:\n",
    "                entry_list.pop(i+1)\n",
    "        else:\n",
    "            i += 1\n",
    "    print(\"processed\", entry_list)\n",
    "    return clean_results(entry_list)\n",
    "\n",
    "\n",
    "\n",
    "#dict_names = [\"AE\", \"AP90\", \"MW\", \"MWE\"]\n",
    "\n",
    "#path = \"/resources/Sanskrit_dictionaries/\"\n",
    "path = \"/Users/jack/Desktop/SanskritData/Sanskrit_dictionaries\"\n",
    "\n",
    "def multidict(name: str, *args: str, source: str = \"MW\") -> Dict[str, List[Any]]:\n",
    "    \n",
    "    dict_names: List[str] = []\n",
    "    dict_results: Dict[str, List[Any]] = {}\n",
    "    \n",
    "    # Check if any arguments were provided\n",
    "    if not args:\n",
    "        dict_names.append(source)  # If no args, use the source (default is \"MW\")\n",
    "    else:\n",
    "        for dict_name in args:\n",
    "            dict_names.append(dict_name)\n",
    "    \n",
    "    # For each dictionary name, build the path and query\n",
    "    for dict_name in dict_names:\n",
    "        path_builder = \"sqlite:///\" + path + dict_name + \".sqlite\"\n",
    "        print(path_builder)\n",
    "        \n",
    "        # Create SQLAlchemy engine\n",
    "        engine = create_engine(path_builder)\n",
    "        \n",
    "        query_builder = f\"\"\"\n",
    "        SELECT data FROM {dict_name} \n",
    "        WHERE key = :name \n",
    "        OR key LIKE :wildcard_name\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare the wildcard by taking the word minus the last letter and appending '_'\n",
    "        wildcard_name = f\"{name[:-1]}_\"\n",
    "\n",
    "        # Execute the query with both the exact match and the wildcard condition\n",
    "        with engine.connect() as connection:\n",
    "            results = connection.execute(\n",
    "                text(query_builder), \n",
    "                {\"name\": name, \"wildcard_name\": wildcard_name}\n",
    "            ).fetchall()\n",
    "\n",
    "\n",
    "        # Step 2 and Step 3 combined: If no exact match, search for name + _ and name minus last letter + _\n",
    "        if not results and len(name) > 1:\n",
    "            query_builder = f\"\"\"\n",
    "            SELECT data FROM {dict_name} \n",
    "            WHERE key LIKE :name1 \n",
    "            OR key LIKE :name2\n",
    "            \"\"\"\n",
    "            with engine.connect() as connection:\n",
    "                results = connection.execute(text(query_builder), {\"name1\": name + \"_\", \"name2\": name[:-1] + \"_\"}).fetchall()\n",
    "\n",
    "        # Add the results to the dict_results with the dictionary name as the key\n",
    "        dict_results[dict_name] = [dict(row) for row in results]\n",
    "    \n",
    "    return dict_results  # Return the dictionary containing all results\n",
    "\n",
    "# Example usage\n",
    "#results = multidict(\"yoga\", \"MW\" \"AP90\")\n",
    "#print(results)\n",
    "\n",
    "\n",
    "\n",
    "def get_voc_entry(list_of_entries, *args: str, source: str = \"MW\"):\n",
    "    entries = []\n",
    "    for entry in list_of_entries:        \n",
    "        if isinstance(entry, list):\n",
    "            \n",
    "            word = entry[0]\n",
    "            dict_entry = []\n",
    "            if word in mwdictionaryKeys:  # Check if the key exists ## check non nel dizionario, ma solo nella lista chiavi\n",
    "                key2 = dictionary_json[word][0][1] ##qui si dovrebbe fare un fetch SQL \n",
    "                key2 = transliterateSLP1IAST(key2) ## qui ho un leggero problema, se prendo l'equivalente in SQL da dove prendo la key 2? dalla prima o dalla seconda voce. Possibilmente dalla seconda se c'è più di una voce. \n",
    "    \n",
    "                for entry_dict in dictionary_json[word]:\n",
    "                    dict_entry.append(re.sub(r'<s>(.*?)</s>', lambda m: '<s>' + transliterateSLP1IAST(m.group(1)) + '</s>', entry_dict[4]))\n",
    "                entry.append(key2)\n",
    "                entry.append(dict_entry)\n",
    "            else:\n",
    "                entry.append(word)  # Append the original word for key2\n",
    "                entry.append([word])  # Append the original word for dict_entry\n",
    "            entries.append(entry)\n",
    "            \n",
    "        elif isinstance(entry, str):\n",
    "            \n",
    "            dict_entry = []\n",
    "            if entry in mwdictionaryKeys:  # Check if the key exists\n",
    "                key2 = dictionary_json[entry][0][1]\n",
    "                key2 = transliterateSLP1IAST(key2)\n",
    "    \n",
    "                for entry_dict in dictionary_json[entry]:\n",
    "                    dict_entry.append(re.sub(r'<s>(.*?)</s>', lambda m: '<s>' + transliterateSLP1IAST(m.group(1)) + '</s>', entry_dict[4]))\n",
    "                entry = [entry]    \n",
    "                \n",
    "                entry.append(key2)\n",
    "                entry.append(dict_entry)\n",
    "            else:\n",
    "                entry = [entry, entry, [entry]]  # Append the original word for key2 and dict_entry\n",
    "            entries.append(entry)\n",
    "    return entries\n",
    "\n",
    "# find_inflection = False, inflection_table = False, \n",
    "#split_compounds = True, dictionary_search = False,\n",
    "##first attempt to process the word not using the sandhi_splitter, which often gives uncorrect;\n",
    "##then if the word is not found, try to split the word in its components and find the root of each component\n",
    "\n",
    "\n",
    "\n",
    "def get_voc_entry(list_of_entries):\n",
    "    entries = []\n",
    "    for entry in list_of_entries:        \n",
    "        if isinstance(entry, list):\n",
    "            \n",
    "            word = entry[0]\n",
    "            dict_entry = []\n",
    "            if word in dictionary_json:  # Check if the key exists ## check non nel dizionario, ma solo nella lista chiavi\n",
    "                key2 = dictionary_json[word][0][1] ##qui si dovrebbe fare un fetch SQL \n",
    "                key2 = transliterateSLP1IAST(key2) ## qui ho un leggero problema, se prendo l'equivalente in SQL da dove prendo la key 2? dalla prima o dalla seconda voce. Possibilmente dalla seconda se c'è più di una voce. \n",
    "    \n",
    "                for entry_dict in dictionary_json[word]:\n",
    "                    dict_entry.append(re.sub(r'<s>(.*?)</s>', lambda m: '<s>' + transliterateSLP1IAST(m.group(1)) + '</s>', entry_dict[4]))\n",
    "                entry.append(key2)\n",
    "                entry.append(dict_entry)\n",
    "            else:\n",
    "                entry.append(word)  # Append the original word for key2\n",
    "                entry.append([word])  # Append the original word for dict_entry\n",
    "            entries.append(entry)\n",
    "            \n",
    "        elif isinstance(entry, str):\n",
    "            \n",
    "            dict_entry = []\n",
    "            if entry in dictionary_json:  # Check if the key exists\n",
    "                key2 = dictionary_json[entry][0][1]\n",
    "                key2 = transliterateSLP1IAST(key2)\n",
    "    \n",
    "                for entry_dict in dictionary_json[entry]:\n",
    "                    dict_entry.append(re.sub(r'<s>(.*?)</s>', lambda m: '<s>' + transliterateSLP1IAST(m.group(1)) + '</s>', entry_dict[4]))\n",
    "                entry = [entry]    \n",
    "                \n",
    "                entry.append(key2)\n",
    "                entry.append(dict_entry)\n",
    "            else:\n",
    "                entry = [entry, entry, [entry]]  # Append the original word for key2 and dict_entry\n",
    "            entries.append(entry)\n",
    "    return entries\n",
    "\n",
    "# find_inflection = False, inflection_table = False, \n",
    "#split_compounds = True, dictionary_search = False,\n",
    "##first attempt to process the word not using the sandhi_splitter, which often gives uncorrect;\n",
    "##then if the word is not found, try to split the word in its components and find the root of each component\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b6f2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['śiva',\n",
       "  'masculine noun/adjective ending in a',\n",
       "  [('Nom', 'Sg')],\n",
       "  ['śivaḥ',\n",
       "   'śivau',\n",
       "   'śivāḥ',\n",
       "   'śivam',\n",
       "   'śivau',\n",
       "   'śivān',\n",
       "   'śivena',\n",
       "   'śivābhyām',\n",
       "   'śivaiḥ',\n",
       "   'śivāya',\n",
       "   'śivābhyām',\n",
       "   'śivebhyaḥ',\n",
       "   'śivāt',\n",
       "   'śivābhyām',\n",
       "   'śivebhyaḥ',\n",
       "   'śivasya',\n",
       "   'śivayoḥ',\n",
       "   'śivānām',\n",
       "   'śive',\n",
       "   'śivayoḥ',\n",
       "   'śiveṣu',\n",
       "   'śiva',\n",
       "   'śivau',\n",
       "   'śivāḥ'],\n",
       "  'SivaH',\n",
       "  'śiva/',\n",
       "  ['<s>śiva/</s>   <lex>mf(<s>ā/</s>)n.</lex> (according to, <ls>Uṇādi-sūtra i, 153</ls>, from √ <hom>1.</hom> <s>śī</s>, ‘in whom all things lie’; perhaps connected with √ <s>śvi</s> <ab>cf.</ab> <s>śavas</s>, <s>śiśvi</s>) auspicious, propitious, gracious, favourable, benign, kind, benevolent, friendly, dear (<s>°va/m</s> <lex type=\"phw\">ind.</lex> kindly, tenderly), Ṛg-veda &amp;c. &amp;c.<info phwchild=\"217543.1\" /><info lex=\"m:f#A:n\" />',\n",
       "   '  happy, fortunate, Bhāgavata-purāṇa<info lex=\"inh\" />',\n",
       "   '<s>śiva/</s>   <lex>m.</lex> happiness, welfare (<ab>cf.</ab> <lex type=\"hwinfo\">n.</lex>), <ls>Rāmāyaṇa v, 56, 36</ls><info lex=\"m\" />',\n",
       "   '<s>śiva/</s>   <lex>m.</lex> liberation, final emancipation, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"m\" />',\n",
       "   '  ‘The Auspicious one’, Name (also = title or epithet) of the disintegrating or destroying and reproducing deity (who constitutes the third god of the <ns>Hindū</ns> <s1 slp1=\"trimUrti\">Trimūrti</s1> or Triad, the other two being <s1 slp1=\"brahmA\">Brahmā</s1> ‘the creator’ and <s1 slp1=\"vizRu\">Viṣṇu</s1> ‘the preserver’; in the <s1 slp1=\"veda\">Veda</s1> the only Name (also = title or epithet) of the destroying deity was <s1 slp1=\"rudra\">Rudra</s1> ‘the terrible god’, but in later times it became usual to give that god the euphemistic Name (also = title or epithet) <s1 slp1=\"Siva\">Śiva</s1> ‘the auspicious’ [just as the Furies were called <gk>Εὐμενίδες</gk> ‘the gracious ones’], and to assign him the office of creation and reproduction as well as dissolution; in fact the preferential worship of <s1 slp1=\"Siva\">Śiva</s1> as developed in the <s1 slp1=\"purARa\">Purāṇa</s1>s and Epic poems led to his being identified with the Supreme Being by his exclusive worshippers [called <s1 slp1=\"SEva\">Śaiva</s1>s]; in his character of destroyer he is sometimes called <s1 slp1=\"kAla\">Kāla</s1> ‘black’, and is then also identified with ‘Time’, although his active destroying function is then oftener assigned to his wife under her name <s1 slp1=\"kAlI\">Kālī</s1>, whose formidable character makes her a general object of propitiation by sacrifices; as presiding over reproduction consequent on destruction <s1 slp1=\"Siva\">Śiva</s1>\\'s symbol is the <s1 slp1=\"liNga\">Liṅga</s1> [quod vide, which see] or Phallus, under which form he is worshipped all over India at the present day; again one of his representations is as <s1 slp1=\"arDa-nArI\">Ardha-nārī</s1>, ‘half-female’, the other half being male to symbolize the unity of the generative principle [<ls>Religious Thought and Life in India, also called \\'Brāhmanism and Hindūism,\\' by Sir M. Monier-Williams 85</ls>]; he has three eyes, one of which is in his forehead, and which are thought to denote his view of the three divisions of time, past, present, and future, while a moon\\'s crescent, above the central eye, marks the measure of time by months, a serpent round his neck the measure by years, and a second necklace of skulls with other serpents about his person, the perpetual revolution of ages, and the successive extinction and generation of the races of mankind: his hair is thickly matted together, and gathered above his forehead into a coil; on the top of it he bears the Ganges, the rush of which in its descent from heaven he intercepted by his head that the earth might not be crushed by the weight of the falling stream; his throat is dark-blue from the stain of the deadly poison which would have destroyed the world had it not been swallowed by him on its production at the churning of the ocean by the gods for the nectar of immortality; he holds a <s>tri-śūla</s>, or three-pronged trident [also called <s1 slp1=\"pinAka\">Pināka</s1>] in his hand to denote, as some think, his combination of the three attributes of Creator, Destroyer, and Regenerator; he also carries a kind of drum, shaped like an hour-glass, called <s1 slp1=\"qamaru\">Ḍamaru</s1>: his attendants or servants are called <s1 slp1=\"pramaTa\">Pramatha</s1> [quod vide, which see]; they are regarded as demons or supernatural beings of different kinds, and form various hosts or troops called <s1 slp1=\"gaRa\">Gaṇa</s1>s; his wife <s1 slp1=\"durgA\">Durgā</s1> [otherwise called <s1 slp1=\"kAlI\">Kālī</s1>, <s1 slp1=\"pArvatI\">Pārvatī</s1>, <s1 slp1=\"umA\">Umā</s1>, <s1 slp1=\"gOrI\">Gaurī</s1>, <s1 slp1=\"BavARI\">Bhavāṇī</s1> &amp;c.] is the chief object of worship with the <s1 slp1=\"SAkta\">Śākta</s1>s and <s1 slp1=\"tAntrika\">Tāntrika</s1>s, and in this connection he is fond of dancing [see <s>tāṇḍava</s>] and wine-drinking <pb n=\"1074,2\" />; he is also worshipped as a great ascetic and is said to have scorched the god of love (<s1 slp1=\"kAma-deva\">Kāma-deva</s1>) to ashes by a glance from his central eye, that deity having attempted to inflame him with passion for <s1 slp1=\"pArvatI\">Pārvatī</s1> whilst he was engaged in severe penance; in the exercise of his function of Universal Destroyer he is fabled to have burnt up the Universe and all the gods, including <s1 slp1=\"brahmA\">Brahmā</s1> and <s1 slp1=\"vizRu\">Viṣṇu</s1>, by a similar scorching glance, and to have rubbed the resulting ashes upon his body, whence the use of ashes in his worship, while the use of the <s1 slp1=\"rudrAkza\">Rudrākṣa</s1> berries originated, it is said, from the legend that <s1 slp1=\"Siva\">Śiva</s1>, on his way to destroy the three cities, called <s1 slp1=\"tri-pura\">Tri-pura</s1>, let fall some tears of rage which became converted into these beads: his residence or heaven is <s1 slp1=\"kElAsa\">Kailāsa</s1>, one of the loftiest northern peaks of the <s1 slp1=\"himAlaya\">Himālaya</s1>; he has strictly no incarnations like those of <s1 slp1=\"vizRu\">Viṣṇu</s1>, though <s1 slp1=\"vIra-Badra\">Vīra-bhadra</s1> and the eight <s1 slp1=\"BErava\">Bhairava</s1>s and <s1 slp1=\"KaRqobA\">Khaṇḍo-bā</s1> &amp;c. [<ls>RTL. 266</ls>] are sometimes regarded as forms of him; he is especially worshipped at Benares and has even more names than <s1 slp1=\"vizRu\">Viṣṇu</s1>, one thousand and eight being specified in the 69th chapter of the <s1 slp1=\"Siva-purARa\">Śiva-Purāṇa</s1> and in the 17th chapter of the <s1 slp1=\"anuSAsana-parvan\">Anuśāsana-parvan</s1> of the <s1 slp1=\"mahA-BArata\">Mahā-bhārata</s1>, some of the most common being <s1 slp1=\"mahA-deva\">Mahā-deva</s1>, <s1 slp1=\"SamBu\">Śambhu</s1>, <s1 slp1=\"SaMkara\">Śaṃkara</s1>, <s1 slp1=\"ISa\">Īśa</s1>, <s1 slp1=\"ISvara\">Īśvara</s1>, <s1 slp1=\"maheSvara\">Maheśvara</s1>, <s1 slp1=\"hara\">Hara</s1>; his sons are <s1 slp1=\"gaReSa\">Gaṇeśa</s1> and <s1 slp1=\"kArttikeya\">Kārttikeya</s1>), Āśvalāyana-śrauta-sūtra; Mahābhārata; Kāvya literature &amp;c., <ls>RTL. 73</ls><info lex=\"inh\" />',\n",
       "   '  a kind of second <s1 slp1=\"Siva\">Śiva</s1> (with <s1 slp1=\"SEva\">Śaiva</s1>s), a person who has attained a particular stage of perfection or emancipation, Mahābhārata; Sarvadarśana-saṃgraha<info lex=\"inh\" />',\n",
       "   '  <s>śiva-liṅga</s>, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  any god, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '<s>śiva/</s>   <lex>m.</lex> a euphemistic Name (also = title or epithet) of a jackal (generally <s>śivā</s> <lex type=\"nhw\">f.</lex> quod vide, which see)<info lex=\"m\" />',\n",
       "   '<s>śiva/</s>   <lex>m.</lex> sacred writings, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"m\" />',\n",
       "   '  (in astronomy) Name (also = title or epithet) of the sixth month<info lex=\"inh\" />',\n",
       "   '  a post for cows (to which they are tied or for them to rub against), Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  bdellium, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  the fragrant bark of <bot>Feronia Elephantum</bot>, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  <bot>Marsilia Dentata</bot>, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  a kind of thorn-apple or = <s>puṇḍarīka</s> (the tree), Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  quicksilver, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc. (<ab>cf.</ab> <s>śiva-bīja</s>)<info lex=\"inh\" />',\n",
       "   '  a particular auspicious constellation, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  a demon who inflicts diseases, Harivaṃśa<info lex=\"inh\" />',\n",
       "   '<s>śiva/</s>   <lex>m.</lex> = <s>śukra</s> <lex type=\"nhw\">m.</lex> <s>kāla</s> <lex type=\"nhw\">m.</lex> <s>vasu</s> <lex type=\"nhw\">m.</lex>, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"m\" />',\n",
       "   '<s>śiva/</s>   <lex>m.</lex> the swift antelope, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"m\" />',\n",
       "   '  rum, spirit distilled from molasses, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  buttermilk, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  a ruby, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  a peg, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  time, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  Name (also = title or epithet) of a son of <s1 slp1=\"meDAtiTi\">Medhātithi</s1>, Mārkaṇḍeya-purāṇa<info lex=\"inh\" />',\n",
       "   '  of a son of <s1 slp1=\"iDma-jihva\">Idhma-jihva</s1>, Bhāgavata-purāṇa<info lex=\"inh\" />',\n",
       "   '  of a prince and various authors (also with <s>dīkṣita</s>, <s>bhaṭṭa</s>, <s>paṇḍita</s>, <s>yajvan</s>, <s>sūri</s> &amp;c.), Catalogue(s) [Cologne Addition]<info lex=\"inh\" />',\n",
       "   '  of a fraudulent person, Kathāsaritsāgara<info lex=\"inh\" />',\n",
       "   '  (dual number) the god <s1 slp1=\"Siva\">Śiva</s1> and his wife, <ls>Kirātārjunīya v, 40</ls>; <ls>Pracaṇḍ. i, 20</ls> (<ab>cf.</ab> <ls>Vām. v, 2, 1</ls>)<info lex=\"inh\" />',\n",
       "   '  plural number Name (also = title or epithet) of a class of gods in the third <s1 slp1=\"manvantara\">Manvantara</s1>, Purāṇas<info lex=\"inh\" />',\n",
       "   '  of a class of <ns>Brāhmans</ns> who have attained a particular degree of perfection like that of <s1 slp1=\"Siva\">Śiva</s1>, Mahābhārata<info lex=\"inh\" />',\n",
       "   '<s>śiva/</s>   <lex>n.</lex> welfare, prosperity, bliss (<s>āya</s>, <s>e/na</s> or <s>e/bhis</s>, ‘auspiciously, fortunately, happily, luckily’; <s>śivāya gamyatām</s>, ‘a prosperous journey to you!’), Ṛg-veda &amp;c. &amp;c.<info lex=\"n\" />',\n",
       "   '  final emancipation, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  water, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  rock-salt, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  sea-salt, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  a kind of borax, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  iron, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  <bot>myrobalan</bot>, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  <bot>Tabernaemontana Coronaria</bot>, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  sandal, Lexicographers, esp. such as Amarasiṃha, Halāyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  Name (also = title or epithet) of a <s1 slp1=\"purARa\">Purāṇa</s1> (= <s>śiva-purāṇa</s> or <s>śaiva</s>), Catalogue(s) [Cologne Addition]<info lex=\"inh\" />',\n",
       "   '  of the house in which the <s1 slp1=\"pARqava\">Pāṇḍava</s1>s were to be burnt, Mārkaṇḍeya-purāṇa<info lex=\"inh\" />',\n",
       "   '  of a <s1 slp1=\"varza\">Varṣa</s1> in <s1 slp1=\"plakza-dvIpa\">Plakṣa-dvīpa</s1> and in <s1 slp1=\"jambu-dvIpa\">Jambu-dvīpa</s1>, Purāṇas<info lex=\"inh\" />']]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\"zivaH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0917fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_any_word(\"zivaH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "201a86b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['yoga',\n",
       "  'masculine noun/adjective ending in a',\n",
       "  [('Gen', 'Pl')],\n",
       "  ['yogaḥ',\n",
       "   'yogau',\n",
       "   'yogāḥ',\n",
       "   'yogam',\n",
       "   'yogau',\n",
       "   'yogān',\n",
       "   'yogena',\n",
       "   'yogābhyām',\n",
       "   'yogaiḥ',\n",
       "   'yogāya',\n",
       "   'yogābhyām',\n",
       "   'yogebhyaḥ',\n",
       "   'yogāt',\n",
       "   'yogābhyām',\n",
       "   'yogebhyaḥ',\n",
       "   'yogasya',\n",
       "   'yogayoḥ',\n",
       "   'yogānām',\n",
       "   'yoge',\n",
       "   'yogayoḥ',\n",
       "   'yogeṣu',\n",
       "   'yoga',\n",
       "   'yogau',\n",
       "   'yogāḥ'],\n",
       "  'yogAnAm'],\n",
       " ['yoga',\n",
       "  'feminine noun/adjective ending in A',\n",
       "  [('Gen', 'Pl')],\n",
       "  ['yogā',\n",
       "   'yoge',\n",
       "   'yogāḥ',\n",
       "   'yogām',\n",
       "   'yoge',\n",
       "   'yogāḥ',\n",
       "   'yogayā',\n",
       "   'yogābhyām',\n",
       "   'yogābhiḥ',\n",
       "   'yogāyai',\n",
       "   'yogābhyām',\n",
       "   'yogābhyaḥ',\n",
       "   'yogāyāḥ',\n",
       "   'yogābhyām',\n",
       "   'yogābhyaḥ',\n",
       "   'yogāyāḥ',\n",
       "   'yogayoḥ',\n",
       "   'yogānām',\n",
       "   'yogāyām',\n",
       "   'yogayoḥ',\n",
       "   'yogāsu',\n",
       "   'yoge',\n",
       "   'yoge',\n",
       "   'yogāḥ'],\n",
       "  'yogAnAm']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_any_word(\"yogAnAm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ce0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
