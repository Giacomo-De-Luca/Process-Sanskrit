{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d802a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import indic_transliteration\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate\n",
    "import ast\n",
    "from detectTransliteration import detect\n",
    "import logging\n",
    "import json\n",
    "import sqlite3\n",
    "import re\n",
    "import pandas as pd\n",
    "from sanskrit_parser import Parser\n",
    "from tabulate import tabulate\n",
    "import regex\n",
    "from itertools import groupby\n",
    "import unicodedata\n",
    "from typing import List, Dict, Any\n",
    "from sqlalchemy import create_engine, text, Column, String\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "## If heroku postgres\n",
    "#load_dotenv()\n",
    "#DATABASE_URL = os.environ['DATABASE_URL']\n",
    "\n",
    "#if local postgres\n",
    "\n",
    "#DATABASE_URL = \"postgresql+psycopg2://postgres:again@localhost:5432/sanskritmagicdb\"\n",
    "#DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "#DATABASE_URL = os.getenv(\"postgres://u5o7c326q19pvp:pdb08c4f74fd1c63df61a559fc3d7a261ac3c65a24df6cfd06fbb2ad511143f0d@c3cj4hehegopde.cluster-czrs8kj4isg7.us-east-1.rds.amazonaws.com:5432/d61ijmaljbh829\")\n",
    "\n",
    "#if DATABASE_URL.startswith(\"postgres://\"):\n",
    "    #DATABASE_URL = DATABASE_URL.replace(\"postgres://\", \"postgresql://\", 1)\n",
    "\n",
    "#if using SQLite\n",
    "\n",
    "DATABASE_URL = \"sqlite:///resources/merged_formdb.sqlite\"\n",
    "\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "##to get all the available schemes\n",
    "##indic_transliteration.sanscript.SCHEMES.keys()\n",
    "    \n",
    "def transliterateSLP1IAST(text):\n",
    "    return transliterate(text, sanscript.SLP1, sanscript.IAST)   \n",
    "\n",
    "def transliterateSLP1HK(text):\n",
    "    return transliterate(text, sanscript.SLP1, sanscript.HK)   \n",
    "\n",
    "def transliterateDEVSLP1(text):\n",
    "    return transliterate(text, sanscript.DEVANAGARI, sanscript.SLP1)\n",
    "        \n",
    "def anythingToSLP1(text):\n",
    "    detected_scheme_str = detect(text).upper()\n",
    "    detected_scheme = getattr(sanscript, detected_scheme_str)\n",
    "    return sanscript.transliterate(text, detected_scheme, sanscript.SLP1)\n",
    "def anythingToIAST(text):\n",
    "    detected_scheme_str = detect(text).upper()\n",
    "    detected_scheme = getattr(sanscript, detected_scheme_str)\n",
    "    return sanscript.transliterate(text, detected_scheme, sanscript.IAST)\n",
    "def anythingToHK(text):\n",
    "    detected_scheme_str = detect(text).upper()\n",
    "    detected_scheme = getattr(sanscript, detected_scheme_str)\n",
    "    return sanscript.transliterate(text, detected_scheme, sanscript.HK)\n",
    "\n",
    "def transliterateAnything(text, transliteration_scheme):\n",
    "    detected_scheme_str = detect(text).upper()\n",
    "    transliteration_scheme_str = transliteration_scheme.upper()\n",
    "    detected_scheme = getattr(sanscript, detected_scheme_str)\n",
    "    output_scheme = getattr(sanscript, transliteration_scheme_str)\n",
    "    return sanscript.transliterate(text, detected_scheme, output_scheme)\n",
    "\n",
    "##qui la lista degli encoding Ã¨ lowercase\n",
    "\n",
    "\n",
    "parser = Parser(output_encoding='slp1')\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Session = sessionmaker(bind=engine)\n",
    "# Create a session\n",
    "session = Session()\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define the split_cache model\n",
    "class SplitCache(Base):\n",
    "    __tablename__ = 'split_cache'\n",
    "    \n",
    "    input = Column(String, primary_key=True)\n",
    "    splitted_text = Column(String)\n",
    "\n",
    "# Create the table if it doesn't exist\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "\n",
    "def sandhi_splitter(text_to_split):\n",
    "    \"\"\"\n",
    "    Splits the given text using a sandhi splitter parser.\n",
    "    Checks if the result is already cached before performing the split.\n",
    "\n",
    "    Parameters:\n",
    "    - text_to_split (str): The text to split.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of split parts of the text.\n",
    "    \"\"\"\n",
    "\n",
    "    #text_to_split = anythingToSLP1(text_to_split).strip()\n",
    "\n",
    "    # Check if the result is already cached\n",
    "    cached_result = session.query(SplitCache).filter_by(input=text_to_split).first()\n",
    "    \n",
    "    if cached_result:\n",
    "        # Retrieve and return the cached result if it exists\n",
    "        splitted_text = ast.literal_eval(cached_result.splitted_text)\n",
    "        print(f\"Retrieved from cache: {splitted_text}\")\n",
    "        return splitted_text\n",
    "\n",
    "    # If not cached, perform the split\n",
    "    try:\n",
    "        splits = parser.split(text_to_split, limit=1)\n",
    "\n",
    "        #if split is none, default to split by space\n",
    "        if splits is None:\n",
    "            return text_to_split.split()\n",
    "        for split in splits:\n",
    "            splitted_text = f'{split}'\n",
    "        splitted_text = ast.literal_eval(splitted_text)\n",
    "\n",
    "        print(f\"Splitted text: {splitted_text}\")\n",
    "\n",
    "        # Store the split result in cache as a list\n",
    "        new_cache_entry = SplitCache(input=text_to_split, splitted_text=str(splitted_text))\n",
    "        session.add(new_cache_entry)\n",
    "        session.commit()\n",
    "        print(f\"Added to cache: {splitted_text}\")\n",
    "\n",
    "        return splitted_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not split the line: {text_to_split}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return text_to_split.split()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stopwords = pd.read_csv('resources/stopwords.csv')\n",
    "\n",
    "stopwords_as_list = stopwords['stopword'].tolist()\n",
    "\n",
    "def remove_stopwords_list(text_list):\n",
    "    return [word for word in text_list if word not in stopwords_as_list]\n",
    "\n",
    "def remove_stopwords_string(text):\n",
    "    text = text.replace('.', '')  # Remove periods\n",
    "    text_list = text.split()  # Split the string into words\n",
    "    return ' '.join(word for word in text_list if word not in stopwords_as_list)\n",
    "\n",
    "\n",
    "with open('resources/MWKeysOnly.json', 'r', encoding='utf-8') as f:\n",
    "    mwdictionaryKeys = json.load(f)\n",
    "\n",
    "##given a name finds the root\n",
    "\n",
    "\n",
    "def SQLite_find_name(name):\n",
    "\n",
    "    outcome = []    \n",
    "\n",
    "    def query1(word):\n",
    "\n",
    "        session = Session()\n",
    "        try:\n",
    "            query_builder = text(\"SELECT * FROM lgtab2 WHERE key = :word\")\n",
    "            results = session.execute(query_builder, {'word': word}).fetchall()\n",
    "        except Exception as error:\n",
    "            print(\"Error while querying PostgreSQL:\", error)\n",
    "            results = []\n",
    "        finally:\n",
    "            session.close()\n",
    "        return results\n",
    "\n",
    "    results = query1(name)\n",
    "    \n",
    "    if not results:  # If query1 didn't find any results\n",
    "        if name[-1] == 'M':\n",
    "            name = name[:-1] + 'm'\n",
    "            results = query1(name)\n",
    "    \n",
    "    for inflected_form, type, root_form in results: \n",
    "        if not root_form:  # If root_form is None or empty\n",
    "            return  # End the function\n",
    "\n",
    "        def query2(root_form: str, type: str):\n",
    "\n",
    "            session = Session()\n",
    "            try:\n",
    "                query_builder2 = \"SELECT * FROM lgtab1 WHERE stem = :root_form and model = :type \"\n",
    "                results = session.execute(query_builder2, {'root_form': root_form, 'type': type}).fetchall()\n",
    "            except Exception as error:\n",
    "                print(\"Error while querying PostgreSQL:\", error)\n",
    "                results = []\n",
    "            finally:\n",
    "                session.close()\n",
    "            return results\n",
    "        \n",
    "        result = query2(root_form, type)\n",
    "        word_refs = re.findall(r\",([a-zA-Z]+)\",result[0][2])[0]\n",
    "        inflection_tuple = result[0][3]  # Get the first element of the first tuple\n",
    "        inflection_words = inflection_tuple.split(':') \n",
    "\n",
    "        ##transliterate back the result to IAST for readability\n",
    "        inflection_wordsIAST = [transliterateSLP1IAST(word) for word in inflection_words]\n",
    "        query_transliterateIAST = transliterateSLP1IAST(name)\n",
    "\n",
    "        ##make Inflection Table\n",
    "        indices = [i for i, x in enumerate(inflection_wordsIAST) if x == query_transliterateIAST]\n",
    "        rowtitles = [\"Nom\", \"Acc\", \"Inst\", \"Dat\", \"Abl\", \"Gen\", \"Loc\", \"Voc\"]\n",
    "        coltitles = [\"Sg\", \"Du\", \"Pl\"]\n",
    "\n",
    "        if indices:\n",
    "            row_col_names = [(rowtitles[i//3], coltitles[i%3]) for i in indices]\n",
    "        else: \n",
    "            row_col_names = None\n",
    "        outcome.append([word_refs, type, row_col_names, inflection_wordsIAST, name])\n",
    "\n",
    "    return outcome\n",
    "\n",
    "\n",
    "\n",
    "def SQLite_find_verb(verb):\n",
    "    \n",
    "    root_form = None\n",
    "\n",
    "    def query1(verb):\n",
    "\n",
    "        session = Session()\n",
    "        try:\n",
    "            query_builder = \"SELECT * FROM vlgtab2 WHERE key = :verb\"\n",
    "            results = session.execute(query_builder, {'verb': verb}).fetchall()\n",
    "        except Exception as error:\n",
    "            print(\"Error while querying PostgreSQL:\", error)\n",
    "            results = []\n",
    "        finally:\n",
    "            session.close()\n",
    "        return results\n",
    "\n",
    "    result = query1(verb)\n",
    "    \n",
    "    for inflected_form, type, root_form in result:\n",
    "\n",
    "        if not root_form:  # If root_form is None or empty\n",
    "            return  # End the function\n",
    "        type_var = type\n",
    "\n",
    "        def query2(root_form: str, type: str):\n",
    "\n",
    "            session = Session()\n",
    "            try:\n",
    "                query_builder2 = \"SELECT * FROM vlgtab1 WHERE stem = :root_form and model = :type\"\n",
    "                results = session.execute(query_builder2, {'root_form': root_form, 'type': type}).fetchall()\n",
    "            except Exception as error:\n",
    "                print(\"Error while querying PostgreSQL:\", error)\n",
    "                results = []\n",
    "            finally:\n",
    "                session.close()\n",
    "            return results\n",
    "        \n",
    "        result = query2(root_form, type)\n",
    "    \n",
    "    selected_tuple = None\n",
    "\n",
    "    # Iterate over the result list\n",
    "    for model, stem, refs, data in result:\n",
    "        if model == type_var:  # If the model matches type_var\n",
    "            ref_word = re.search(\",([a-zA-Z]+)\", refs).group(1)\n",
    "            if stem != ref_word:\n",
    "                stem= ref_word\n",
    "                #print(\"ref_word, stem\", ref_word, stem)\n",
    "                selected_tuple = (model, stem, refs, data)  # Get the entire tuple\n",
    "                break  # Exit the loop\n",
    "            selected_tuple = (model, stem, refs, data)  # Get the entire tuple\n",
    "            break  # Exit the loop\n",
    "\n",
    "    if selected_tuple is None:\n",
    "        #print(\"No matching model found in result\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    # Now you can use selected_tuple\n",
    "    inflection_tuple = selected_tuple[3]  # Get the 'data' element of the tuple\n",
    "    inflection_words = inflection_tuple.split(':') \n",
    "    \n",
    "    ##transliterate back the result to IAST for readability\n",
    "    \n",
    "    inflection_wordsIAST = [transliterateSLP1IAST(word) for word in inflection_words]\n",
    "    query_transliterateIAST = transliterateSLP1IAST(verb)\n",
    "    \n",
    "    ##make Inflection Table\n",
    "    \n",
    "    indices = [i for i, x in enumerate(inflection_wordsIAST) if x == query_transliterateIAST]\n",
    "\n",
    "    # Define row and column titles\n",
    "    rowtitles = [\"First\", \"Second\", \"Third\"]\n",
    "    coltitles = [\"Sg\", \"Du\", \"Pl\"]\n",
    "\n",
    "\n",
    "    if indices:\n",
    "        row_col_names = [(rowtitles[i//3], coltitles[i%3]) for i in indices]\n",
    "    else:\n",
    "        row_col_names = None\n",
    "        \n",
    "    return [[stem, type_var, row_col_names, inflection_wordsIAST, verb]]\n",
    "\n",
    "\n",
    "## also map to the type.\n",
    "# Read the Excel file into a DataFrame\n",
    "type_map = pd.read_excel('resources/type_map.xlsx')\n",
    "\n",
    "def root_any_word(word):\n",
    "\n",
    "\n",
    "    result_roots_name = SQLite_find_name(word)  \n",
    "    result_roots_verb = SQLite_find_verb(word) \n",
    "\n",
    "\n",
    "    if result_roots_name:\n",
    "        if result_roots_verb:\n",
    "            result_roots = result_roots_name + result_roots_verb\n",
    "        else:\n",
    "            result_roots = result_roots_name\n",
    "    else:\n",
    "        result_roots = result_roots_verb\n",
    "\n",
    "    if result_roots:\n",
    "        for i in range(len(result_roots)):\n",
    "            result = result_roots[i]\n",
    "            # Get the second member of the list\n",
    "            abbr = result[1]\n",
    "            # Find the matching value in the 'abbr' column\n",
    "            match = type_map[type_map['abbr'] == abbr]\n",
    "            \n",
    "            if not match.empty:\n",
    "                description = match['description'].values[0]\n",
    "                result[1] = description\n",
    "                result_roots[i] = result\n",
    "        return result_roots\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def root_any_list(text_list):\n",
    "    roots = []\n",
    "    for word in text_list:\n",
    "        root_found = root_any_word(word)\n",
    "        if root_found is not None:\n",
    "            roots.append(root_found)\n",
    "    \n",
    "    # Transliterate the first element of each tuple\n",
    "    for i in range(len(roots)):\n",
    "        roots[i] = (transliterateSLP1IAST(roots[i][0].replace('-', '')),) + roots[i][1:]\n",
    "    \n",
    "    return roots\n",
    "\n",
    "\n",
    "def dict_word_iterative(word):\n",
    "    sanskrit_end_letters = ['a', 'Ä', 'i', 'Ä«', 'u', 'Å«', 'e', 's']  # Add all Sanskrit end-of-word letters here\n",
    "    temp_word = word\n",
    "    removed_part = ''  # Part of the word that was removed\n",
    "    found = False  # Flag to indicate when a match has been found\n",
    "    if temp_word in mwdictionaryKeys:  \n",
    "            found_word = temp_word \n",
    "            found = True\n",
    "            return [found_word, removed_part]\n",
    "    while temp_word and not found: # Continue the loop until a match is found or temp_word is empty        \n",
    "        if temp_word[:-1] in mwdictionaryKeys:  \n",
    "            found_word = temp_word[:-1] \n",
    "            #print(temp_word[:-1] ) debug\n",
    "            found = True\n",
    "            return [found_word, removed_part]       \n",
    "        elif temp_word[-1].isalpha():  # If the last character is a letter\n",
    "            for letter in sanskrit_end_letters:\n",
    "                attempt = temp_word[:-1] + letter\n",
    "                #print(attempt) debug\n",
    "                if attempt in mwdictionaryKeys:\n",
    "                    found_word = attempt\n",
    "                    found = True  # Set the flag to True when a match is found\n",
    "                    break                                \n",
    "        removed_part = temp_word[-1] + removed_part  # Keep track of the removed part\n",
    "        temp_word = temp_word[:-1]  # Remove the last character, regardless of whether it's a letter or not\n",
    "    \n",
    "    return [found_word, removed_part] if found else None  # Return the match if found, else return None\n",
    "\n",
    "\n",
    "\n",
    "        ##if the dictionary approach fails, try the iterative approach:\n",
    "\n",
    "\n",
    "def root_compounds(word):\n",
    "\n",
    "    if word[0] == \"'\":\n",
    "        word = 'a' + word[1:]\n",
    "\n",
    "    first_root = dict_word_iterative(word)\n",
    "    #print(\"first_root\", first_root)\n",
    "    first_root_entry = root_any_word(first_root[0])\n",
    "    #print(\"first_root_entry\", first_root_entry)\n",
    "    \n",
    "    ## if it's a compound\n",
    "    if first_root is not None and first_root[1] is not None and len(first_root[1]) >= 4:\n",
    "        \n",
    "        ##remove the first root from the word\n",
    "        without_root = word.replace(first_root[0], '', 1)  # Only replace the first occurrence\n",
    "        \n",
    "        ##try the dictionary approach\n",
    "        second_root = root_any_word(without_root)\n",
    "        \n",
    "        ##if the dictionary approach fails, try the iterative approach:\n",
    "        if second_root == None:\n",
    "            second_root = dict_word_iterative(without_root)\n",
    "            #print(\"second_root\", second_root)\n",
    "            if second_root is None:\n",
    "                if first_root_entry is not None:\n",
    "                    return first_root_entry\n",
    "                else:\n",
    "                    return [first_root[0]]\n",
    "            if len(second_root[0]) < 2:\n",
    "                second_root = None\n",
    "            else:\n",
    "                second_root_try = root_any_word(second_root[0])\n",
    "                if second_root_try is not None:\n",
    "                    second_root = second_root_try\n",
    "                else: \n",
    "                    second_root = [second_root[0]]            \n",
    "            if second_root is not None:\n",
    "                if first_root_entry is not None:                    \n",
    "                    return first_root_entry + second_root\n",
    "                else:\n",
    "                    return [first_root[0]] + second_root\n",
    "            else:\n",
    "                if first_root_entry is not None:\n",
    "                    return first_root_entry\n",
    "                else:\n",
    "                    return [first_root[0]]\n",
    "        else:\n",
    "            if first_root_entry is not None:\n",
    "                return first_root_entry + second_root\n",
    "            else:\n",
    "                return [first_root[0]] + second_root\n",
    "            \n",
    "    ## if it's not a compound        \n",
    "    else:\n",
    "        if first_root_entry is not None:\n",
    "            return first_root_entry\n",
    "        else:\n",
    "            return [first_root[0]]\n",
    "            \n",
    "\n",
    "\n",
    "def root_compounds(word):\n",
    "    if word.startswith(\"'\"):\n",
    "        word = 'a' + word[1:]\n",
    "\n",
    "    # Base case: if word is empty, return empty list\n",
    "    if not word:\n",
    "        return []\n",
    "\n",
    "    roots = []\n",
    "\n",
    "    first_root = dict_word_iterative(word)\n",
    "    if not first_root or not first_root[0]:\n",
    "        # No root found\n",
    "        return []\n",
    "\n",
    "    first_root_entry = root_any_word(first_root[0])\n",
    "    if first_root_entry is not None:\n",
    "        if isinstance(first_root_entry, list):\n",
    "            roots.extend(first_root_entry)\n",
    "        else:\n",
    "            roots.append(first_root_entry)\n",
    "    else:\n",
    "        roots.append(first_root[0])\n",
    "\n",
    "    # Remove the first root from the word\n",
    "    without_root = word.replace(first_root[0], '', 1)\n",
    "\n",
    "    # If there's nothing left, return the roots found\n",
    "    if not without_root:\n",
    "        return roots\n",
    "\n",
    "    # Now, recursively process the remaining word\n",
    "    rest_entries = root_any_word(without_root)\n",
    "    if rest_entries is not None:\n",
    "        if isinstance(rest_entries, list):\n",
    "            roots.extend(rest_entries)\n",
    "        else:\n",
    "            roots.append(rest_entries)\n",
    "    else:\n",
    "        # Try dict_word_iterative and ensure result is longer than 2\n",
    "        rest_root = dict_word_iterative(without_root)\n",
    "        if rest_root is None or len(rest_root[0]) < 2:\n",
    "            # Cannot proceed further; return roots found so far\n",
    "            return roots\n",
    "        else:\n",
    "            # Recursively process the rest of the word\n",
    "            rest_roots = root_compounds(without_root)\n",
    "            roots.extend(rest_roots)\n",
    "\n",
    "    return roots\n",
    "\n",
    "\n",
    "def root_compounds2(word, entries=None, depth=0, max_depth=5):\n",
    "    \"\"\"\n",
    "    Recursively analyze compound words to find their constituents.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word to analyze\n",
    "        entries (list): Accumulated entries\n",
    "        depth (int): Current recursion depth\n",
    "        max_depth (int): Maximum recursion depth to prevent infinite loops\n",
    "        \n",
    "    Returns:\n",
    "        list: List of root constituents found in the word\n",
    "    \"\"\"\n",
    "    # Initialize entries list if None\n",
    "    if entries is None:\n",
    "        entries = []\n",
    "    \n",
    "    # Base cases\n",
    "    if depth >= max_depth or not word:\n",
    "        return entries\n",
    "    \n",
    "    # Handle special case with apostrophe\n",
    "    if word[0] == \"'\":\n",
    "        word = 'a' + word[1:]\n",
    "    \n",
    "    # Get first root using iterative approach\n",
    "    first_root = dict_word_iterative(word)\n",
    "    if first_root is None:\n",
    "        return entries\n",
    "    \n",
    "    # Get morphological analysis of the first root\n",
    "    first_root_entry = root_any_word(first_root[0])\n",
    "    \n",
    "    # Store first root entry or first root\n",
    "    if first_root_entry is not None:\n",
    "        entries.extend(first_root_entry)\n",
    "    else:\n",
    "        entries.append(first_root[0])\n",
    "    \n",
    "    # If the remainder is too short, return accumulated entries\n",
    "    if first_root[1] is None or len(first_root[1]) < 4:\n",
    "        return entries\n",
    "    \n",
    "    # Remove the first root from the word\n",
    "    remaining = word.replace(first_root[0], '', 1)\n",
    "    \n",
    "    # Try to analyze the remaining part\n",
    "    def analyze_remaining(remaining_word):\n",
    "        nonlocal entries\n",
    "        \n",
    "        # First try dictionary approach\n",
    "        remaining_root = root_any_word(remaining_word)\n",
    "        if remaining_root is not None:\n",
    "            entries.extend(remaining_root)\n",
    "            return True\n",
    "        \n",
    "        # If dictionary approach fails, try iterative approach\n",
    "        iterative_result = dict_word_iterative(remaining_word)\n",
    "        if iterative_result is None or len(iterative_result[0]) < 2:\n",
    "            return False\n",
    "            \n",
    "        # Try to get morphological analysis of the iterative result\n",
    "        remaining_root = root_any_word(iterative_result[0])\n",
    "        if remaining_root is not None:\n",
    "            entries.extend(remaining_root)\n",
    "            # Recursively analyze any remaining parts\n",
    "            if iterative_result[1]:\n",
    "                root_compounds2(iterative_result[1], entries, depth + 1, max_depth)\n",
    "            return True\n",
    "        \n",
    "        # If no morphological analysis, use the iterative result\n",
    "        entries.append(iterative_result[0])\n",
    "        # Recursively analyze any remaining parts\n",
    "        if iterative_result[1]:\n",
    "            root_compounds2(iterative_result[1], entries, depth + 1, max_depth)\n",
    "        return True\n",
    "    \n",
    "    # Analyze the remaining part\n",
    "    analyze_remaining(remaining)\n",
    "    return entries\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inflect(splitted_text):\n",
    "    roots = []\n",
    "    \n",
    "#    print(\"splitted\", splitted_text)\n",
    "    for word in splitted_text: \n",
    "        \n",
    "        rooted = root_any_word(word)\n",
    "        if rooted is not None:\n",
    "            for root in rooted:\n",
    "                roots.append(root)  \n",
    "        else:\n",
    "            print(\"here breaks\", word)\n",
    "            compound_try = root_compounds(word)\n",
    "            if compound_try is not None:\n",
    "                roots.extend(compound_try)  \n",
    "                #print(\"compound_try\", compound_try)\n",
    "            else:\n",
    "                roots.extend(word)  \n",
    "                \n",
    "    for i in range(len(roots)):\n",
    "        if isinstance(roots[i], list):\n",
    "            #print(\"debug\", roots)\n",
    "            #print(roots[i][0])\n",
    "            roots[i][0] = transliterateSLP1IAST(roots[i][0].replace('-', ''))\n",
    "        else:\n",
    "            #print(roots[i][0])\n",
    "            roots[i] = transliterateSLP1IAST(roots[i].replace('-', ''))           \n",
    "    #print(\"inflect roots\", roots)\n",
    "    return roots             \n",
    "\n",
    "## bug with process(\"nÄ«lotpalapatrÄyatÄká¹£Ä«\")    \n",
    "\n",
    "\n",
    "with open('resources/no_abbreviationMW.json') as f:\n",
    "    # Load JSON data from file\n",
    "    dictionary_json = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "def process(text):\n",
    "\n",
    "    if ' ' not in text:\n",
    "        #print(\"single_word\", text)\n",
    "        transliterated_text = anythingToSLP1(text)     \n",
    "        #print(\"transliterated_text\", transliterated_text)\n",
    "        text = regex.sub('[^\\p{L}\\']', '', transliterated_text)\n",
    "        ## here it should be transliterated to SLP1 before and added aH at the end instead of a\n",
    "        if text[0] == \"'\":\n",
    "            text = 'a' + text[1:]\n",
    "        if text[-1] == 'o':\n",
    "            text = text[:-1] + 'aH'\n",
    "        #print(\"text\", text)\n",
    "        if \"o'\" in text:\n",
    "            modified_text = re.sub(r\"o'\", \"aH a\", text)\n",
    "            #print(\"modified_text\", modified_text)\n",
    "            result = process(modified_text)\n",
    "            return result\n",
    "        result = root_any_word(text)\n",
    "        if result is None and text[-1] == 'M':\n",
    "            text = text[:-1] + 'm'\n",
    "            result = root_any_word(text)\n",
    "        if result is not None:\n",
    "            for res in result:\n",
    "                if isinstance(res, list):\n",
    "                    res[0] = transliterateSLP1IAST(res[0].replace('-', ''))\n",
    "                    #print(\"res\", res)\n",
    "            result_vocabulary = get_voc_entry(result)  \n",
    "            #print(\"result_vocabulary\", result_vocabulary)\n",
    "            return clean_results(result_vocabulary)\n",
    "        else:\n",
    "            query = [transliterateSLP1IAST(text)]\n",
    "            print(\"query\", query)\n",
    "            result_vocabulary = get_voc_entry(query)  \n",
    "            print(\"result_vocabulary\", result_vocabulary)\n",
    "            if result_vocabulary[0][0] != result_vocabulary[0][2][0]:\n",
    "                return clean_results(result_vocabulary)\n",
    "            \n",
    "    ## attempt to remove sandhi and tokenise in any case\n",
    "    print(\"pre_splitted_text\", text)\n",
    "    splitted_text = sandhi_splitter(text)    \n",
    "    inflections = inflect(splitted_text) \n",
    "    inflections_vocabulary = get_voc_entry(inflections)\n",
    "    inflections_vocabulary = [entry for entry in inflections_vocabulary if len(entry[0]) > 1]       \n",
    "\n",
    "    return clean_results(inflections_vocabulary)\n",
    "\n",
    "\n",
    "##process(\"dveá¹£ÄnuviddhaÅcetanÄcetanasÄdhanÄdhÄ«nastÄpÄnubhava\")\n",
    "\n",
    "filtered_words = [\"ca\", \"na\", \"eva\", \"ni\", \"apya\", \"ava\"]\n",
    "\n",
    "\n",
    "def clean_results(list_of_entries):\n",
    "\n",
    "    i = 0\n",
    "    while i < len(list_of_entries) - 1:  # Subtract 1 to avoid index out of range error\n",
    "        # Check if the word is in filtered_words\n",
    "        if list_of_entries[i][0] in filtered_words:\n",
    "            while i < len(list_of_entries) - 1 and list_of_entries[i + 1][0] == list_of_entries[i][0]:\n",
    "                del list_of_entries[i + 1]\n",
    "        \n",
    "        # Check if the word is \"sam\"\n",
    "        if list_of_entries[i][0] == \"sam\":\n",
    "            j = i + 1\n",
    "            while j < len(list_of_entries) and (list_of_entries[j][0] == \"sa\" or list_of_entries[j][0] == \"sam\"):\n",
    "                j += 1\n",
    "            if j < len(list_of_entries):\n",
    "                voc_entry = get_voc_entry([\"sam\" + list_of_entries[j][0]])\n",
    "                print(\"voc_entry\", voc_entry)\n",
    "                if voc_entry[0][0] == voc_entry[0][2][0]:\n",
    "                    print(\"revised query\", [\"saM\" + list_of_entries[j][0]])\n",
    "                    voc_entry = process(\"saM\" + list_of_entries[j][0])\n",
    "                    print(\"revise_voc_entry\", voc_entry)\n",
    "                if voc_entry is not None:\n",
    "                    list_of_entries[i] = [item for sublist in voc_entry for item in sublist]\n",
    "                    del list_of_entries[i + 1:j + 1]\n",
    "        \n",
    "        # Check if the word is \"anu\"\n",
    "        if list_of_entries[i][0] == \"anu\":\n",
    "            j = i + 1\n",
    "            while j < len(list_of_entries) and (list_of_entries[j][0] == \"anu\"):\n",
    "                j += 1\n",
    "            if j < len(list_of_entries):\n",
    "                voc_entry = get_voc_entry([\"anu\" + list_of_entries[j][0]])\n",
    "                if voc_entry is not None:\n",
    "                    list_of_entries[i] = [item for sublist in voc_entry for item in sublist]\n",
    "                    del list_of_entries[i + 1:j + 1]\n",
    "        \n",
    "        # Check if the word is \"ava\"\n",
    "        if list_of_entries[i][0] == \"ava\":\n",
    "            j = i + 1\n",
    "            while j < len(list_of_entries) and (list_of_entries[j][0] == \"ava\"):\n",
    "                j += 1\n",
    "            if j < len(list_of_entries):\n",
    "                print(\"testing with:\", [\"ava\" + list_of_entries[j + 1][0]])\n",
    "                voc_entry = get_voc_entry([\"ava\" + list_of_entries[j + 1][0]])\n",
    "                if voc_entry is not None:\n",
    "                    list_of_entries[i] = [item for sublist in voc_entry for item in sublist]\n",
    "                    del list_of_entries[i + 1:j + 1]        \n",
    "        i += 1  \n",
    "    return list_of_entries\n",
    "\n",
    "def process_test(text, remove_stopwords = False, dictionary_entry = True, output_encoding = \"IAST\", entry_only = False):\n",
    "    \n",
    "    ## attempt to remove sandhi and tokenise in any case\n",
    "    splitted_text = sandhi_splitter(text)\n",
    "    \n",
    "    ## removes stopwords\n",
    "    if remove_stopwords == True: \n",
    "        splitted_text = remove_stopwords_list(splitted_text)\n",
    "        \n",
    "    \n",
    "    inflections = inflect(splitted_text) \n",
    "    \n",
    "    if dictionary_entry == True:\n",
    "        inflections = get_voc_entry(inflections)    \n",
    "\n",
    "    if entry_only == True:\n",
    "        entry_list = []\n",
    "        for entry in inflections:\n",
    "            entry_list.append(entry[0])\n",
    "        inflections = entry_list    \n",
    "\n",
    "    return clean_results(inflections)\n",
    "\n",
    "## hard mode testing:\n",
    "#process(\"dveá¹£ÄnuviddhaÅcetanÄcetanasÄdhanÄdhÄ«nastÄpÄnubhava\")\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    remove_char_text = ''.join(c for c in text if c.isalpha() or c == \"'\" or c == ' ')\n",
    "\n",
    "    print(\"processing\", text)\n",
    "    ## attempt to remove sandhi and tokenise in any case\n",
    "    splitted_text = sandhi_splitter(remove_char_text)   \n",
    "\n",
    "    splitted_text = remove_stopwords_list(splitted_text)    \n",
    "    \n",
    "    inflections = inflect(splitted_text) \n",
    "    \n",
    "    entry_list = []\n",
    "    entry_list = [entry[0] for entry in inflections if len(entry[0]) > 1]\n",
    "    entry_list = [key for key, group in groupby(entry_list)]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(entry_list) - 1:\n",
    "        # Normalize words to form without diacritics\n",
    "        word1 = unicodedata.normalize('NFD', entry_list[i])\n",
    "        word2 = unicodedata.normalize('NFD', entry_list[i+1])\n",
    "\n",
    "        # Remove diacritics\n",
    "        word1 = ''.join(c for c in word1 if not unicodedata.combining(c))\n",
    "        word2 = ''.join(c for c in word2 if not unicodedata.combining(c))\n",
    "\n",
    "        if word1 in word2 or word2 in word1:\n",
    "            if len(entry_list[i]) > len(entry_list[i+1]):\n",
    "                entry_list.pop(i)\n",
    "            else:\n",
    "                entry_list.pop(i+1)\n",
    "        else:\n",
    "            i += 1\n",
    "    print(\"processed\", entry_list)\n",
    "    return clean_results(entry_list)\n",
    "\n",
    "\n",
    "\n",
    "#dict_names = [\"AE\", \"AP90\", \"MW\", \"MWE\"]\n",
    "\n",
    "#path = \"/resources/Sanskrit_dictionaries/\"\n",
    "path = \"/Users/jack/Desktop/SanskritData/Sanskrit_dictionaries\"\n",
    "\n",
    "def multidict(name: str, *args: str, source: str = \"MW\") -> Dict[str, List[Any]]:\n",
    "    \n",
    "    dict_names: List[str] = []\n",
    "    dict_results: Dict[str, List[Any]] = {}\n",
    "    \n",
    "    # Check if any arguments were provided\n",
    "    if not args:\n",
    "        dict_names.append(source)  # If no args, use the source (default is \"MW\")\n",
    "    else:\n",
    "        for dict_name in args:\n",
    "            dict_names.append(dict_name)\n",
    "    \n",
    "    # For each dictionary name, build the path and query\n",
    "    for dict_name in dict_names:\n",
    "        path_builder = \"sqlite:///\" + path + dict_name + \".sqlite\"\n",
    "        print(path_builder)\n",
    "        \n",
    "        # Create SQLAlchemy engine\n",
    "        engine = create_engine(path_builder)\n",
    "        \n",
    "        query_builder = f\"\"\"\n",
    "        SELECT data FROM {dict_name} \n",
    "        WHERE key = :name \n",
    "        OR key LIKE :wildcard_name\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare the wildcard by taking the word minus the last letter and appending '_'\n",
    "        wildcard_name = f\"{name[:-1]}_\"\n",
    "\n",
    "        # Execute the query with both the exact match and the wildcard condition\n",
    "        with engine.connect() as connection:\n",
    "            results = connection.execute(\n",
    "                text(query_builder), \n",
    "                {\"name\": name, \"wildcard_name\": wildcard_name}\n",
    "            ).fetchall()\n",
    "\n",
    "\n",
    "        # Step 2 and Step 3 combined: If no exact match, search for name + _ and name minus last letter + _\n",
    "        if not results and len(name) > 1:\n",
    "            query_builder = f\"\"\"\n",
    "            SELECT data FROM {dict_name} \n",
    "            WHERE key LIKE :name1 \n",
    "            OR key LIKE :name2\n",
    "            \"\"\"\n",
    "            with engine.connect() as connection:\n",
    "                results = connection.execute(text(query_builder), {\"name1\": name + \"_\", \"name2\": name[:-1] + \"_\"}).fetchall()\n",
    "\n",
    "        # Add the results to the dict_results with the dictionary name as the key\n",
    "        dict_results[dict_name] = [dict(row) for row in results]\n",
    "    \n",
    "    return dict_results  # Return the dictionary containing all results\n",
    "\n",
    "# Example usage\n",
    "#results = multidict(\"yoga\", \"MW\" \"AP90\")\n",
    "#print(results)\n",
    "\n",
    "\n",
    "\n",
    "def get_voc_entry(list_of_entries, *args: str, source: str = \"MW\"):\n",
    "    entries = []\n",
    "    for entry in list_of_entries:        \n",
    "        if isinstance(entry, list):\n",
    "            \n",
    "            word = entry[0]\n",
    "            dict_entry = []\n",
    "            if word in mwdictionaryKeys:  # Check if the key exists ## check non nel dizionario, ma solo nella lista chiavi\n",
    "                key2 = dictionary_json[word][0][1] ##qui si dovrebbe fare un fetch SQL \n",
    "                key2 = transliterateSLP1IAST(key2) ## qui ho un leggero problema, se prendo l'equivalente in SQL da dove prendo la key 2? dalla prima o dalla seconda voce. Possibilmente dalla seconda se c'Ã¨ piÃ¹ di una voce. \n",
    "    \n",
    "                for entry_dict in dictionary_json[word]:\n",
    "                    dict_entry.append(re.sub(r'<s>(.*?)</s>', lambda m: '<s>' + transliterateSLP1IAST(m.group(1)) + '</s>', entry_dict[4]))\n",
    "                entry.append(key2)\n",
    "                entry.append(dict_entry)\n",
    "            else:\n",
    "                entry.append(word)  # Append the original word for key2\n",
    "                entry.append([word])  # Append the original word for dict_entry\n",
    "            entries.append(entry)\n",
    "            \n",
    "        elif isinstance(entry, str):\n",
    "            \n",
    "            dict_entry = []\n",
    "            if entry in mwdictionaryKeys:  # Check if the key exists\n",
    "                key2 = dictionary_json[entry][0][1]\n",
    "                key2 = transliterateSLP1IAST(key2)\n",
    "    \n",
    "                for entry_dict in dictionary_json[entry]:\n",
    "                    dict_entry.append(re.sub(r'<s>(.*?)</s>', lambda m: '<s>' + transliterateSLP1IAST(m.group(1)) + '</s>', entry_dict[4]))\n",
    "                entry = [entry]    \n",
    "                \n",
    "                entry.append(key2)\n",
    "                entry.append(dict_entry)\n",
    "            else:\n",
    "                entry = [entry, entry, [entry]]  # Append the original word for key2 and dict_entry\n",
    "            entries.append(entry)\n",
    "    return entries\n",
    "\n",
    "# find_inflection = False, inflection_table = False, \n",
    "#split_compounds = True, dictionary_search = False,\n",
    "##first attempt to process the word not using the sandhi_splitter, which often gives uncorrect;\n",
    "##then if the word is not found, try to split the word in its components and find the root of each component\n",
    "\n",
    "\n",
    "\n",
    "def get_voc_entry(list_of_entries):\n",
    "    entries = []\n",
    "    for entry in list_of_entries:        \n",
    "        if isinstance(entry, list):\n",
    "            \n",
    "            word = entry[0]\n",
    "            dict_entry = []\n",
    "            if word in dictionary_json:  # Check if the key exists ## check non nel dizionario, ma solo nella lista chiavi\n",
    "                key2 = dictionary_json[word][0][1] ##qui si dovrebbe fare un fetch SQL \n",
    "                key2 = transliterateSLP1IAST(key2) ## qui ho un leggero problema, se prendo l'equivalente in SQL da dove prendo la key 2? dalla prima o dalla seconda voce. Possibilmente dalla seconda se c'Ã¨ piÃ¹ di una voce. \n",
    "    \n",
    "                for entry_dict in dictionary_json[word]:\n",
    "                    dict_entry.append(re.sub(r'<s>(.*?)</s>', lambda m: '<s>' + transliterateSLP1IAST(m.group(1)) + '</s>', entry_dict[4]))\n",
    "                entry.append(key2)\n",
    "                entry.append(dict_entry)\n",
    "            else:\n",
    "                entry.append(word)  # Append the original word for key2\n",
    "                entry.append([word])  # Append the original word for dict_entry\n",
    "            entries.append(entry)\n",
    "            \n",
    "        elif isinstance(entry, str):\n",
    "            \n",
    "            dict_entry = []\n",
    "            if entry in dictionary_json:  # Check if the key exists\n",
    "                key2 = dictionary_json[entry][0][1]\n",
    "                key2 = transliterateSLP1IAST(key2)\n",
    "    \n",
    "                for entry_dict in dictionary_json[entry]:\n",
    "                    dict_entry.append(re.sub(r'<s>(.*?)</s>', lambda m: '<s>' + transliterateSLP1IAST(m.group(1)) + '</s>', entry_dict[4]))\n",
    "                entry = [entry]    \n",
    "                \n",
    "                entry.append(key2)\n",
    "                entry.append(dict_entry)\n",
    "            else:\n",
    "                entry = [entry, entry, [entry]]  # Append the original word for key2 and dict_entry\n",
    "            entries.append(entry)\n",
    "    return entries\n",
    "\n",
    "# find_inflection = False, inflection_table = False, \n",
    "#split_compounds = True, dictionary_search = False,\n",
    "##first attempt to process the word not using the sandhi_splitter, which often gives uncorrect;\n",
    "##then if the word is not found, try to split the word in its components and find the root of each component\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b6f2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Åiva',\n",
       "  'masculine noun/adjective ending in a',\n",
       "  [('Nom', 'Sg')],\n",
       "  ['Åivaá¸¥',\n",
       "   'Åivau',\n",
       "   'ÅivÄá¸¥',\n",
       "   'Åivam',\n",
       "   'Åivau',\n",
       "   'ÅivÄn',\n",
       "   'Åivena',\n",
       "   'ÅivÄbhyÄm',\n",
       "   'Åivaiá¸¥',\n",
       "   'ÅivÄya',\n",
       "   'ÅivÄbhyÄm',\n",
       "   'Åivebhyaá¸¥',\n",
       "   'ÅivÄt',\n",
       "   'ÅivÄbhyÄm',\n",
       "   'Åivebhyaá¸¥',\n",
       "   'Åivasya',\n",
       "   'Åivayoá¸¥',\n",
       "   'ÅivÄnÄm',\n",
       "   'Åive',\n",
       "   'Åivayoá¸¥',\n",
       "   'Åiveá¹£u',\n",
       "   'Åiva',\n",
       "   'Åivau',\n",
       "   'ÅivÄá¸¥'],\n",
       "  'SivaH',\n",
       "  'Åiva/',\n",
       "  ['<s>Åiva/</s>   <lex>mf(<s>Ä/</s>)n.</lex> (according to, <ls>Uá¹Ädi-sÅ«tra i, 153</ls>, from â <hom>1.</hom> <s>ÅÄ«</s>, âin whom all things lieâ; perhaps connected with â <s>Åvi</s> <ab>cf.</ab> <s>Åavas</s>, <s>ÅiÅvi</s>) auspicious, propitious, gracious, favourable, benign, kind, benevolent, friendly, dear (<s>Â°va/m</s> <lex type=\"phw\">ind.</lex> kindly, tenderly), á¹g-veda &amp;c. &amp;c.<info phwchild=\"217543.1\" /><info lex=\"m:f#A:n\" />',\n",
       "   '  happy, fortunate, BhÄgavata-purÄá¹a<info lex=\"inh\" />',\n",
       "   '<s>Åiva/</s>   <lex>m.</lex> happiness, welfare (<ab>cf.</ab> <lex type=\"hwinfo\">n.</lex>), <ls>RÄmÄyaá¹a v, 56, 36</ls><info lex=\"m\" />',\n",
       "   '<s>Åiva/</s>   <lex>m.</lex> liberation, final emancipation, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"m\" />',\n",
       "   '  âThe Auspicious oneâ, Name (also = title or epithet) of the disintegrating or destroying and reproducing deity (who constitutes the third god of the <ns>HindÅ«</ns> <s1 slp1=\"trimUrti\">TrimÅ«rti</s1> or Triad, the other two being <s1 slp1=\"brahmA\">BrahmÄ</s1> âthe creatorâ and <s1 slp1=\"vizRu\">Viá¹£á¹u</s1> âthe preserverâ; in the <s1 slp1=\"veda\">Veda</s1> the only Name (also = title or epithet) of the destroying deity was <s1 slp1=\"rudra\">Rudra</s1> âthe terrible godâ, but in later times it became usual to give that god the euphemistic Name (also = title or epithet) <s1 slp1=\"Siva\">Åiva</s1> âthe auspiciousâ [just as the Furies were called <gk>Îá½Î¼ÎµÎ½á½·Î´ÎµÏ</gk> âthe gracious onesâ], and to assign him the office of creation and reproduction as well as dissolution; in fact the preferential worship of <s1 slp1=\"Siva\">Åiva</s1> as developed in the <s1 slp1=\"purARa\">PurÄá¹a</s1>s and Epic poems led to his being identified with the Supreme Being by his exclusive worshippers [called <s1 slp1=\"SEva\">Åaiva</s1>s]; in his character of destroyer he is sometimes called <s1 slp1=\"kAla\">KÄla</s1> âblackâ, and is then also identified with âTimeâ, although his active destroying function is then oftener assigned to his wife under her name <s1 slp1=\"kAlI\">KÄlÄ«</s1>, whose formidable character makes her a general object of propitiation by sacrifices; as presiding over reproduction consequent on destruction <s1 slp1=\"Siva\">Åiva</s1>\\'s symbol is the <s1 slp1=\"liNga\">Liá¹ga</s1> [quod vide, which see] or Phallus, under which form he is worshipped all over India at the present day; again one of his representations is as <s1 slp1=\"arDa-nArI\">Ardha-nÄrÄ«</s1>, âhalf-femaleâ, the other half being male to symbolize the unity of the generative principle [<ls>Religious Thought and Life in India, also called \\'BrÄhmanism and HindÅ«ism,\\' by Sir M. Monier-Williams 85</ls>]; he has three eyes, one of which is in his forehead, and which are thought to denote his view of the three divisions of time, past, present, and future, while a moon\\'s crescent, above the central eye, marks the measure of time by months, a serpent round his neck the measure by years, and a second necklace of skulls with other serpents about his person, the perpetual revolution of ages, and the successive extinction and generation of the races of mankind: his hair is thickly matted together, and gathered above his forehead into a coil; on the top of it he bears the Ganges, the rush of which in its descent from heaven he intercepted by his head that the earth might not be crushed by the weight of the falling stream; his throat is dark-blue from the stain of the deadly poison which would have destroyed the world had it not been swallowed by him on its production at the churning of the ocean by the gods for the nectar of immortality; he holds a <s>tri-ÅÅ«la</s>, or three-pronged trident [also called <s1 slp1=\"pinAka\">PinÄka</s1>] in his hand to denote, as some think, his combination of the three attributes of Creator, Destroyer, and Regenerator; he also carries a kind of drum, shaped like an hour-glass, called <s1 slp1=\"qamaru\">á¸amaru</s1>: his attendants or servants are called <s1 slp1=\"pramaTa\">Pramatha</s1> [quod vide, which see]; they are regarded as demons or supernatural beings of different kinds, and form various hosts or troops called <s1 slp1=\"gaRa\">Gaá¹a</s1>s; his wife <s1 slp1=\"durgA\">DurgÄ</s1> [otherwise called <s1 slp1=\"kAlI\">KÄlÄ«</s1>, <s1 slp1=\"pArvatI\">PÄrvatÄ«</s1>, <s1 slp1=\"umA\">UmÄ</s1>, <s1 slp1=\"gOrI\">GaurÄ«</s1>, <s1 slp1=\"BavARI\">BhavÄá¹Ä«</s1> &amp;c.] is the chief object of worship with the <s1 slp1=\"SAkta\">ÅÄkta</s1>s and <s1 slp1=\"tAntrika\">TÄntrika</s1>s, and in this connection he is fond of dancing [see <s>tÄá¹á¸ava</s>] and wine-drinking <pb n=\"1074,2\" />; he is also worshipped as a great ascetic and is said to have scorched the god of love (<s1 slp1=\"kAma-deva\">KÄma-deva</s1>) to ashes by a glance from his central eye, that deity having attempted to inflame him with passion for <s1 slp1=\"pArvatI\">PÄrvatÄ«</s1> whilst he was engaged in severe penance; in the exercise of his function of Universal Destroyer he is fabled to have burnt up the Universe and all the gods, including <s1 slp1=\"brahmA\">BrahmÄ</s1> and <s1 slp1=\"vizRu\">Viá¹£á¹u</s1>, by a similar scorching glance, and to have rubbed the resulting ashes upon his body, whence the use of ashes in his worship, while the use of the <s1 slp1=\"rudrAkza\">RudrÄká¹£a</s1> berries originated, it is said, from the legend that <s1 slp1=\"Siva\">Åiva</s1>, on his way to destroy the three cities, called <s1 slp1=\"tri-pura\">Tri-pura</s1>, let fall some tears of rage which became converted into these beads: his residence or heaven is <s1 slp1=\"kElAsa\">KailÄsa</s1>, one of the loftiest northern peaks of the <s1 slp1=\"himAlaya\">HimÄlaya</s1>; he has strictly no incarnations like those of <s1 slp1=\"vizRu\">Viá¹£á¹u</s1>, though <s1 slp1=\"vIra-Badra\">VÄ«ra-bhadra</s1> and the eight <s1 slp1=\"BErava\">Bhairava</s1>s and <s1 slp1=\"KaRqobA\">Khaá¹á¸o-bÄ</s1> &amp;c. [<ls>RTL. 266</ls>] are sometimes regarded as forms of him; he is especially worshipped at Benares and has even more names than <s1 slp1=\"vizRu\">Viá¹£á¹u</s1>, one thousand and eight being specified in the 69th chapter of the <s1 slp1=\"Siva-purARa\">Åiva-PurÄá¹a</s1> and in the 17th chapter of the <s1 slp1=\"anuSAsana-parvan\">AnuÅÄsana-parvan</s1> of the <s1 slp1=\"mahA-BArata\">MahÄ-bhÄrata</s1>, some of the most common being <s1 slp1=\"mahA-deva\">MahÄ-deva</s1>, <s1 slp1=\"SamBu\">Åambhu</s1>, <s1 slp1=\"SaMkara\">Åaá¹kara</s1>, <s1 slp1=\"ISa\">ÄªÅa</s1>, <s1 slp1=\"ISvara\">ÄªÅvara</s1>, <s1 slp1=\"maheSvara\">MaheÅvara</s1>, <s1 slp1=\"hara\">Hara</s1>; his sons are <s1 slp1=\"gaReSa\">Gaá¹eÅa</s1> and <s1 slp1=\"kArttikeya\">KÄrttikeya</s1>), ÄÅvalÄyana-Årauta-sÅ«tra; MahÄbhÄrata; KÄvya literature &amp;c., <ls>RTL. 73</ls><info lex=\"inh\" />',\n",
       "   '  a kind of second <s1 slp1=\"Siva\">Åiva</s1> (with <s1 slp1=\"SEva\">Åaiva</s1>s), a person who has attained a particular stage of perfection or emancipation, MahÄbhÄrata; SarvadarÅana-saá¹graha<info lex=\"inh\" />',\n",
       "   '  <s>Åiva-liá¹ga</s>, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  any god, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '<s>Åiva/</s>   <lex>m.</lex> a euphemistic Name (also = title or epithet) of a jackal (generally <s>ÅivÄ</s> <lex type=\"nhw\">f.</lex> quod vide, which see)<info lex=\"m\" />',\n",
       "   '<s>Åiva/</s>   <lex>m.</lex> sacred writings, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"m\" />',\n",
       "   '  (in astronomy) Name (also = title or epithet) of the sixth month<info lex=\"inh\" />',\n",
       "   '  a post for cows (to which they are tied or for them to rub against), Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  bdellium, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  the fragrant bark of <bot>Feronia Elephantum</bot>, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  <bot>Marsilia Dentata</bot>, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  a kind of thorn-apple or = <s>puá¹á¸arÄ«ka</s> (the tree), Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  quicksilver, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc. (<ab>cf.</ab> <s>Åiva-bÄ«ja</s>)<info lex=\"inh\" />',\n",
       "   '  a particular auspicious constellation, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  a demon who inflicts diseases, Harivaá¹Åa<info lex=\"inh\" />',\n",
       "   '<s>Åiva/</s>   <lex>m.</lex> = <s>Åukra</s> <lex type=\"nhw\">m.</lex> <s>kÄla</s> <lex type=\"nhw\">m.</lex> <s>vasu</s> <lex type=\"nhw\">m.</lex>, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"m\" />',\n",
       "   '<s>Åiva/</s>   <lex>m.</lex> the swift antelope, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"m\" />',\n",
       "   '  rum, spirit distilled from molasses, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  buttermilk, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  a ruby, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  a peg, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  time, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  Name (also = title or epithet) of a son of <s1 slp1=\"meDAtiTi\">MedhÄtithi</s1>, MÄrkaá¹á¸eya-purÄá¹a<info lex=\"inh\" />',\n",
       "   '  of a son of <s1 slp1=\"iDma-jihva\">Idhma-jihva</s1>, BhÄgavata-purÄá¹a<info lex=\"inh\" />',\n",
       "   '  of a prince and various authors (also with <s>dÄ«ká¹£ita</s>, <s>bhaá¹­á¹­a</s>, <s>paá¹á¸ita</s>, <s>yajvan</s>, <s>sÅ«ri</s> &amp;c.), Catalogue(s) [Cologne Addition]<info lex=\"inh\" />',\n",
       "   '  of a fraudulent person, KathÄsaritsÄgara<info lex=\"inh\" />',\n",
       "   '  (dual number) the god <s1 slp1=\"Siva\">Åiva</s1> and his wife, <ls>KirÄtÄrjunÄ«ya v, 40</ls>; <ls>Pracaá¹á¸. i, 20</ls> (<ab>cf.</ab> <ls>VÄm. v, 2, 1</ls>)<info lex=\"inh\" />',\n",
       "   '  plural number Name (also = title or epithet) of a class of gods in the third <s1 slp1=\"manvantara\">Manvantara</s1>, PurÄá¹as<info lex=\"inh\" />',\n",
       "   '  of a class of <ns>BrÄhmans</ns> who have attained a particular degree of perfection like that of <s1 slp1=\"Siva\">Åiva</s1>, MahÄbhÄrata<info lex=\"inh\" />',\n",
       "   '<s>Åiva/</s>   <lex>n.</lex> welfare, prosperity, bliss (<s>Äya</s>, <s>e/na</s> or <s>e/bhis</s>, âauspiciously, fortunately, happily, luckilyâ; <s>ÅivÄya gamyatÄm</s>, âa prosperous journey to you!â), á¹g-veda &amp;c. &amp;c.<info lex=\"n\" />',\n",
       "   '  final emancipation, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  water, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  rock-salt, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  sea-salt, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  a kind of borax, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  iron, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  <bot>myrobalan</bot>, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  <bot>Tabernaemontana Coronaria</bot>, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  sandal, Lexicographers, esp. such as Amarasiá¹ha, HalÄyudha, Hemacandra, etc.<info lex=\"inh\" />',\n",
       "   '  Name (also = title or epithet) of a <s1 slp1=\"purARa\">PurÄá¹a</s1> (= <s>Åiva-purÄá¹a</s> or <s>Åaiva</s>), Catalogue(s) [Cologne Addition]<info lex=\"inh\" />',\n",
       "   '  of the house in which the <s1 slp1=\"pARqava\">PÄá¹á¸ava</s1>s were to be burnt, MÄrkaá¹á¸eya-purÄá¹a<info lex=\"inh\" />',\n",
       "   '  of a <s1 slp1=\"varza\">Vará¹£a</s1> in <s1 slp1=\"plakza-dvIpa\">Plaká¹£a-dvÄ«pa</s1> and in <s1 slp1=\"jambu-dvIpa\">Jambu-dvÄ«pa</s1>, PurÄá¹as<info lex=\"inh\" />']]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\"zivaH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0917fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_any_word(\"zivaH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "201a86b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['yoga',\n",
       "  'masculine noun/adjective ending in a',\n",
       "  [('Gen', 'Pl')],\n",
       "  ['yogaá¸¥',\n",
       "   'yogau',\n",
       "   'yogÄá¸¥',\n",
       "   'yogam',\n",
       "   'yogau',\n",
       "   'yogÄn',\n",
       "   'yogena',\n",
       "   'yogÄbhyÄm',\n",
       "   'yogaiá¸¥',\n",
       "   'yogÄya',\n",
       "   'yogÄbhyÄm',\n",
       "   'yogebhyaá¸¥',\n",
       "   'yogÄt',\n",
       "   'yogÄbhyÄm',\n",
       "   'yogebhyaá¸¥',\n",
       "   'yogasya',\n",
       "   'yogayoá¸¥',\n",
       "   'yogÄnÄm',\n",
       "   'yoge',\n",
       "   'yogayoá¸¥',\n",
       "   'yogeá¹£u',\n",
       "   'yoga',\n",
       "   'yogau',\n",
       "   'yogÄá¸¥'],\n",
       "  'yogAnAm'],\n",
       " ['yoga',\n",
       "  'feminine noun/adjective ending in A',\n",
       "  [('Gen', 'Pl')],\n",
       "  ['yogÄ',\n",
       "   'yoge',\n",
       "   'yogÄá¸¥',\n",
       "   'yogÄm',\n",
       "   'yoge',\n",
       "   'yogÄá¸¥',\n",
       "   'yogayÄ',\n",
       "   'yogÄbhyÄm',\n",
       "   'yogÄbhiá¸¥',\n",
       "   'yogÄyai',\n",
       "   'yogÄbhyÄm',\n",
       "   'yogÄbhyaá¸¥',\n",
       "   'yogÄyÄá¸¥',\n",
       "   'yogÄbhyÄm',\n",
       "   'yogÄbhyaá¸¥',\n",
       "   'yogÄyÄá¸¥',\n",
       "   'yogayoá¸¥',\n",
       "   'yogÄnÄm',\n",
       "   'yogÄyÄm',\n",
       "   'yogayoá¸¥',\n",
       "   'yogÄsu',\n",
       "   'yoge',\n",
       "   'yoge',\n",
       "   'yogÄá¸¥'],\n",
       "  'yogAnAm']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_any_word(\"yogAnAm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ce0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
