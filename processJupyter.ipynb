{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sanskrit_parser'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msanskrit_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parser\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtabulate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tabulate\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sanskrit_parser'"
     ]
    }
   ],
   "source": [
    "import indic_transliteration\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate\n",
    "import ast\n",
    "from detectTransliteration import detect\n",
    "import logging\n",
    "import json\n",
    "import sqlite3\n",
    "import re\n",
    "import pandas as pd\n",
    "from sanskrit_parser import Parser\n",
    "from tabulate import tabulate\n",
    "import regex\n",
    "from itertools import groupby\n",
    "import unicodedata\n",
    "from typing import List, Dict, Any\n",
    "from sqlalchemy import create_engine, text, Column, String\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "\n",
    "\n",
    "__version__ = \"0.1\"\n",
    "def print_version():\n",
    "    print(f\"Version: {__version__}\")\n",
    "\n",
    "\n",
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "## If heroku postgres\n",
    "#load_dotenv()\n",
    "#DATABASE_URL = os.environ['DATABASE_URL']\n",
    "\n",
    "#if local postgres\n",
    "\n",
    "#DATABASE_URL = \"postgresql+psycopg2://postgres:again@localhost:5432/sanskritmagicdb\"\n",
    "#DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "#DATABASE_URL = os.getenv(\"postgres://u5o7c326q19pvp:pdb08c4f74fd1c63df61a559fc3d7a261ac3c65a24df6cfd06fbb2ad511143f0d@c3cj4hehegopde.cluster-czrs8kj4isg7.us-east-1.rds.amazonaws.com:5432/d61ijmaljbh829\")\n",
    "\n",
    "#if DATABASE_URL.startswith(\"postgres://\"):\n",
    "    #DATABASE_URL = DATABASE_URL.replace(\"postgres://\", \"postgresql://\", 1)\n",
    "\n",
    "#if using SQLite\n",
    "\n",
    "DATABASE_URL = \"sqlite:///resources/merged_formdb.sqlite\"\n",
    "\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "##to get all the available schemes\n",
    "##indic_transliteration.sanscript.SCHEMES.keys()\n",
    "    \n",
    "def transliterateSLP1IAST(text):\n",
    "    return transliterate(text, sanscript.SLP1, sanscript.IAST)   \n",
    "def transliterateIASTSLP1(text):\n",
    "    return transliterate(text, sanscript.IAST, sanscript.SLP1)   \n",
    "\n",
    "\n",
    "def transliterateSLP1HK(text):\n",
    "    return transliterate(text, sanscript.SLP1, sanscript.HK)   \n",
    "\n",
    "def transliterateDEVSLP1(text):\n",
    "    return transliterate(text, sanscript.DEVANAGARI, sanscript.SLP1)\n",
    "        \n",
    "def anythingToSLP1(text):\n",
    "    detected_scheme_str = detect(text).upper()\n",
    "    detected_scheme = getattr(sanscript, detected_scheme_str)\n",
    "    return sanscript.transliterate(text, detected_scheme, sanscript.SLP1)\n",
    "def anythingToIAST(text):\n",
    "    detected_scheme_str = detect(text).upper()\n",
    "    detected_scheme = getattr(sanscript, detected_scheme_str)\n",
    "    return sanscript.transliterate(text, detected_scheme, sanscript.IAST)\n",
    "def anythingToHK(text):\n",
    "    detected_scheme_str = detect(text).upper()\n",
    "    detected_scheme = getattr(sanscript, detected_scheme_str)\n",
    "    return sanscript.transliterate(text, detected_scheme, sanscript.HK)\n",
    "\n",
    "def transliterateAnything(text, transliteration_scheme):\n",
    "    detected_scheme_str = detect(text).upper()\n",
    "    transliteration_scheme_str = transliteration_scheme.upper()\n",
    "    detected_scheme = getattr(sanscript, detected_scheme_str)\n",
    "    output_scheme = getattr(sanscript, transliteration_scheme_str)\n",
    "    return sanscript.transliterate(text, detected_scheme, output_scheme)\n",
    "\n",
    "##qui la lista degli encoding Ã¨ lowercase\n",
    "\n",
    "\n",
    "parser = Parser(output_encoding='iast')\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Session = sessionmaker(bind=engine)\n",
    "# Create a session\n",
    "session = Session()\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define the split_cache model\n",
    "class SplitCache(Base):\n",
    "    __tablename__ = 'split_cache'\n",
    "    \n",
    "    input = Column(String, primary_key=True)\n",
    "    splitted_text = Column(String)\n",
    "\n",
    "# Create the table if it doesn't exist\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "\n",
    "def sandhi_splitter(text_to_split):\n",
    "    \"\"\"\n",
    "    Splits the given text using a sandhi splitter parser.\n",
    "    Checks if the result is already cached before performing the split.\n",
    "\n",
    "    Parameters:\n",
    "    - text_to_split (str): The text to split.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of split parts of the text.\n",
    "    \"\"\"\n",
    "\n",
    "    #text_to_split = anythingToSLP1(text_to_split).strip()\n",
    "\n",
    "    # Check if the result is already cached\n",
    "    cached_result = session.query(SplitCache).filter_by(input=text_to_split).first()\n",
    "    \n",
    "    if cached_result:\n",
    "        # Retrieve and return the cached result if it exists\n",
    "        splitted_text = ast.literal_eval(cached_result.splitted_text)\n",
    "        print(f\"Retrieved from cache: {splitted_text}\")\n",
    "        return splitted_text\n",
    "\n",
    "    # If not cached, perform the split\n",
    "    try:\n",
    "        splits = parser.split(text_to_split, limit=1)\n",
    "\n",
    "        #if split is none, default to split by space\n",
    "        if splits is None:\n",
    "            return text_to_split.split()\n",
    "        for split in splits:\n",
    "            splitted_text = f'{split}'\n",
    "        splitted_text = ast.literal_eval(splitted_text)\n",
    "\n",
    "        print(f\"Splitted text: {splitted_text}\")\n",
    "\n",
    "        # Store the split result in cache as a list\n",
    "        new_cache_entry = SplitCache(input=text_to_split, splitted_text=str(splitted_text))\n",
    "        session.add(new_cache_entry)\n",
    "        session.commit()\n",
    "        print(f\"Added to cache: {splitted_text}\")\n",
    "\n",
    "        return splitted_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not split the line: {text_to_split}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return text_to_split.split()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stopwords = pd.read_csv('resources/stopwords.csv')\n",
    "\n",
    "stopwords_as_list = stopwords['stopword'].tolist()\n",
    "\n",
    "def remove_stopwords_list(text_list):\n",
    "    return [word for word in text_list if word not in stopwords_as_list]\n",
    "\n",
    "def remove_stopwords_string(text):\n",
    "    text = text.replace('.', '')  # Remove periods\n",
    "    text_list = text.split()  # Split the string into words\n",
    "    return ' '.join(word for word in text_list if word not in stopwords_as_list)\n",
    "\n",
    "\n",
    "with open('resources/MWKeysOnly.json', 'r', encoding='utf-8') as f:\n",
    "    mwdictionaryKeys = json.load(f)\n",
    "\n",
    "with open('resources/MWKeysOnlySLP1.json', 'r', encoding='utf-8') as f:\n",
    "    mwdictionaryKeysSLP1 = json.load(f)\n",
    "\n",
    "\n",
    "##given a name finds the root\n",
    "\n",
    "\n",
    "def SQLite_find_name(name):\n",
    "\n",
    "    outcome = []    \n",
    "\n",
    "    def query1(word):\n",
    "\n",
    "        session = Session()\n",
    "        try:\n",
    "            query_builder = text(\"SELECT * FROM lgtab2 WHERE key = :word\")\n",
    "            results = session.execute(query_builder, {'word': word}).fetchall()\n",
    "        except Exception as error:\n",
    "            print(\"Error while querying PostgreSQL:\", error)\n",
    "            results = []\n",
    "        finally:\n",
    "            session.close()\n",
    "        return results\n",
    "\n",
    "    results = query1(name)\n",
    "    \n",
    "    if not results:  # If query1 didn't find any results\n",
    "        if name[-1] == 'M':\n",
    "            name = name[:-1] + 'm'\n",
    "            results = query1(name)\n",
    "    \n",
    "    for inflected_form, type, root_form in results: \n",
    "        if not root_form:  # If root_form is None or empty\n",
    "            return  # End the function\n",
    "\n",
    "        def query2(root_form: str, type: str):\n",
    "\n",
    "            session = Session()\n",
    "            try:\n",
    "                query_builder2 = \"SELECT * FROM lgtab1 WHERE stem = :root_form and model = :type \"\n",
    "                results = session.execute(query_builder2, {'root_form': root_form, 'type': type}).fetchall()\n",
    "            except Exception as error:\n",
    "                print(\"Error while querying PostgreSQL:\", error)\n",
    "                results = []\n",
    "            finally:\n",
    "                session.close()\n",
    "            return results\n",
    "        \n",
    "        result = query2(root_form, type)\n",
    "        word_refs = re.findall(r\",([a-zA-Z]+)\",result[0][2])[0]\n",
    "        inflection_tuple = result[0][3]  # Get the first element of the first tuple\n",
    "        inflection_words = inflection_tuple.split(':') \n",
    "\n",
    "        ##transliterate back the result to IAST for readability\n",
    "        inflection_wordsIAST = [transliterateSLP1IAST(word) for word in inflection_words]\n",
    "        query_transliterateIAST = transliterateSLP1IAST(name)\n",
    "\n",
    "        ##make Inflection Table\n",
    "        indices = [i for i, x in enumerate(inflection_wordsIAST) if x == query_transliterateIAST]\n",
    "        rowtitles = [\"Nom\", \"Acc\", \"Inst\", \"Dat\", \"Abl\", \"Gen\", \"Loc\", \"Voc\"]\n",
    "        coltitles = [\"Sg\", \"Du\", \"Pl\"]\n",
    "\n",
    "        if indices:\n",
    "            row_col_names = [(rowtitles[i//3], coltitles[i%3]) for i in indices]\n",
    "        else: \n",
    "            row_col_names = None\n",
    "        outcome.append([word_refs, type, row_col_names, inflection_wordsIAST, transliterateSLP1IAST(name)])\n",
    "\n",
    "    return outcome\n",
    "\n",
    "\n",
    "\n",
    "def SQLite_find_verb(verb):\n",
    "    \n",
    "    root_form = None\n",
    "\n",
    "    def query1(verb):\n",
    "\n",
    "        session = Session()\n",
    "        try:\n",
    "            query_builder = \"SELECT * FROM vlgtab2 WHERE key = :verb\"\n",
    "            results = session.execute(query_builder, {'verb': verb}).fetchall()\n",
    "        except Exception as error:\n",
    "            print(\"Error while querying PostgreSQL:\", error)\n",
    "            results = []\n",
    "        finally:\n",
    "            session.close()\n",
    "        return results\n",
    "\n",
    "    result = query1(verb)\n",
    "    \n",
    "    for inflected_form, type, root_form in result:\n",
    "\n",
    "        if not root_form:  # If root_form is None or empty\n",
    "            return  # End the function\n",
    "        type_var = type\n",
    "\n",
    "        def query2(root_form: str, type: str):\n",
    "\n",
    "            session = Session()\n",
    "            try:\n",
    "                query_builder2 = \"SELECT * FROM vlgtab1 WHERE stem = :root_form and model = :type\"\n",
    "                results = session.execute(query_builder2, {'root_form': root_form, 'type': type}).fetchall()\n",
    "            except Exception as error:\n",
    "                print(\"Error while querying PostgreSQL:\", error)\n",
    "                results = []\n",
    "            finally:\n",
    "                session.close()\n",
    "            return results\n",
    "        \n",
    "        result = query2(root_form, type)\n",
    "    \n",
    "    selected_tuple = None\n",
    "\n",
    "    # Iterate over the result list\n",
    "    for model, stem, refs, data in result:\n",
    "        if model == type_var:  # If the model matches type_var\n",
    "            ref_word = re.search(\",([a-zA-Z]+)\", refs).group(1)\n",
    "            if stem != ref_word:\n",
    "                stem= ref_word\n",
    "                #print(\"ref_word, stem\", ref_word, stem)\n",
    "                selected_tuple = (model, stem, refs, data)  # Get the entire tuple\n",
    "                break  # Exit the loop\n",
    "            selected_tuple = (model, stem, refs, data)  # Get the entire tuple\n",
    "            break  # Exit the loop\n",
    "\n",
    "    if selected_tuple is None:\n",
    "        #print(\"No matching model found in result\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    # Now you can use selected_tuple\n",
    "    inflection_tuple = selected_tuple[3]  # Get the 'data' element of the tuple\n",
    "    inflection_words = inflection_tuple.split(':') \n",
    "    \n",
    "    ##transliterate back the result to IAST for readability\n",
    "    \n",
    "    inflection_wordsIAST = [transliterateSLP1IAST(word) for word in inflection_words]\n",
    "    query_transliterateIAST = transliterateSLP1IAST(verb)\n",
    "    \n",
    "    ##make Inflection Table\n",
    "    \n",
    "    indices = [i for i, x in enumerate(inflection_wordsIAST) if x == query_transliterateIAST]\n",
    "\n",
    "    # Define row and column titles\n",
    "    rowtitles = [\"First\", \"Second\", \"Third\"]\n",
    "    coltitles = [\"Sg\", \"Du\", \"Pl\"]\n",
    "\n",
    "\n",
    "    if indices:\n",
    "        row_col_names = [(rowtitles[i//3], coltitles[i%3]) for i in indices]\n",
    "    else:\n",
    "        row_col_names = None\n",
    "        \n",
    "    return [[stem, type_var, row_col_names, inflection_wordsIAST, transliterateSLP1IAST(verb)]]\n",
    "\n",
    "\n",
    "## also map to the type.\n",
    "# Read the Excel file into a DataFrame\n",
    "type_map = pd.read_excel('resources/type_map.xlsx')\n",
    "\n",
    "def root_any_word(word):\n",
    "\n",
    "\n",
    "    result_roots_name = SQLite_find_name(word)  \n",
    "    result_roots_verb = SQLite_find_verb(word) \n",
    "\n",
    "\n",
    "    if result_roots_name:\n",
    "        if result_roots_verb:\n",
    "            result_roots = result_roots_name + result_roots_verb\n",
    "        else:\n",
    "            result_roots = result_roots_name\n",
    "    else:\n",
    "        result_roots = result_roots_verb\n",
    "\n",
    "    if result_roots:\n",
    "        for i in range(len(result_roots)):\n",
    "            result = result_roots[i]\n",
    "            # Get the second member of the list\n",
    "            abbr = result[1]\n",
    "            # Find the matching value in the 'abbr' column\n",
    "            match = type_map[type_map['abbr'] == abbr]\n",
    "            \n",
    "            if not match.empty:\n",
    "                description = match['description'].values[0]\n",
    "                result[1] = description\n",
    "                result_roots[i] = result\n",
    "        return result_roots\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def root_any_list(text_list):\n",
    "    roots = []\n",
    "    for word in text_list:\n",
    "        root_found = root_any_word(word)\n",
    "        if root_found is not None:\n",
    "            roots.append(root_found)\n",
    "    \n",
    "    # Transliterate the first element of each tuple\n",
    "    for i in range(len(roots)):\n",
    "        roots[i] = (transliterateSLP1IAST(roots[i][0].replace('-', '')),) + roots[i][1:]\n",
    "    \n",
    "    return roots\n",
    "\n",
    "\n",
    "def dict_word_iterativeIAST(word):\n",
    "    sanskrit_end_letters = ['a', 'Ä', 'i', 'Ä«', 'u', 'Å«', 'e', 's']  # Add all Sanskrit end-of-word letters here\n",
    "    temp_word = word\n",
    "    removed_part = ''  # Part of the word that was removed\n",
    "    found = False  # Flag to indicate when a match has been found\n",
    "    if temp_word in mwdictionaryKeys:  \n",
    "            found_word = temp_word \n",
    "            found = True\n",
    "            return [found_word, removed_part]\n",
    "    while temp_word and not found: # Continue the loop until a match is found or temp_word is empty        \n",
    "        if temp_word[:-1] in mwdictionaryKeys:  \n",
    "            found_word = temp_word[:-1] \n",
    "            #print(temp_word[:-1] ) debug\n",
    "            found = True\n",
    "            return [found_word, removed_part]       \n",
    "        elif temp_word[-1].isalpha():  # If the last character is a letter\n",
    "            for letter in sanskrit_end_letters:\n",
    "                attempt = temp_word[:-1] + letter\n",
    "                #print(attempt) debug\n",
    "                if attempt in mwdictionaryKeys:\n",
    "                    found_word = attempt\n",
    "                    found = True  # Set the flag to True when a match is found\n",
    "                    break                                \n",
    "        removed_part = temp_word[-1] + removed_part  # Keep track of the removed part\n",
    "        temp_word = temp_word[:-1]  # Remove the last character, regardless of whether it's a letter or not\n",
    "    \n",
    "    return [found_word, removed_part] if found else None  # Return the match if found, else return None\n",
    "\n",
    "def dict_word_iterative(word):\n",
    "    sanskrit_end_letters = [\"a\", \"A\", \"i\", \"I\", \"u\", \"U\", \"f\", \"d\", \"n\", \"t\", \"m\", \"s\", \"h\"]  # Add all Sanskrit end-of-word letters here\n",
    "    temp_word = word\n",
    "    removed_part = ''  # Part of the word that was removed\n",
    "    found = False  # Flag to indicate when a match has been found\n",
    "    if temp_word in mwdictionaryKeysSLP1:  \n",
    "            found_word = temp_word \n",
    "            found = True\n",
    "            return [found_word, removed_part]\n",
    "    while temp_word and not found: # Continue the loop until a match is found or temp_word is empty        \n",
    "        if temp_word[:-1] in mwdictionaryKeysSLP1:  \n",
    "            found_word = temp_word[:-1] \n",
    "            #print(temp_word[:-1] ) debug\n",
    "            found = True\n",
    "            return [found_word, removed_part]       \n",
    "        elif temp_word[-1].isalpha():  # If the last character is a letter\n",
    "            for letter in sanskrit_end_letters:\n",
    "                attempt = temp_word[:-1] + letter\n",
    "                #print(attempt) debug\n",
    "                if attempt in mwdictionaryKeysSLP1:\n",
    "                    found_word = attempt\n",
    "                    found = True  # Set the flag to True when a match is found\n",
    "                    break                                \n",
    "        removed_part = temp_word[-1] + removed_part  # Keep track of the removed part\n",
    "        temp_word = temp_word[:-1]  # Remove the last character, regardless of whether it's a letter or not\n",
    "    \n",
    "    return [found_word, removed_part] if found else None  # Return the match if found, else return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ##if the dictionary approach fails, try the iterative approach:\n",
    "\n",
    "\n",
    "def root_compounds(word):\n",
    "\n",
    "    if word[0] == \"'\":\n",
    "        word = 'a' + word[1:]\n",
    "\n",
    "    first_root = dict_word_iterative(word)\n",
    "    #print(\"first_root\", first_root)\n",
    "    first_root_entry = root_any_word(first_root[0])\n",
    "    #print(\"first_root_entry\", first_root_entry)\n",
    "    \n",
    "    ## if it's a compound\n",
    "    if first_root is not None and first_root[1] is not None and len(first_root[1]) >= 4:\n",
    "        \n",
    "        ##remove the first root from the word\n",
    "        without_root = word.replace(first_root[0], '', 1)  # Only replace the first occurrence\n",
    "        \n",
    "        ##try the dictionary approach\n",
    "        second_root = root_any_word(without_root)\n",
    "        \n",
    "        ##if the dictionary approach fails, try the iterative approach:\n",
    "        if second_root == None:\n",
    "            second_root = dict_word_iterative(without_root)\n",
    "            #print(\"second_root\", second_root)\n",
    "            if second_root is None:\n",
    "                if first_root_entry is not None:\n",
    "                    return first_root_entry\n",
    "                else:\n",
    "                    return [first_root[0]]\n",
    "            if len(second_root[0]) < 2:\n",
    "                second_root = None\n",
    "            else:\n",
    "                second_root_try = root_any_word(second_root[0])\n",
    "                if second_root_try is not None:\n",
    "                    second_root = second_root_try\n",
    "                else: \n",
    "                    second_root = [second_root[0]]            \n",
    "            if second_root is not None:\n",
    "                if first_root_entry is not None:                    \n",
    "                    return first_root_entry + second_root\n",
    "                else:\n",
    "                    return [first_root[0]] + second_root\n",
    "            else:\n",
    "                if first_root_entry is not None:\n",
    "                    return first_root_entry\n",
    "                else:\n",
    "                    return [first_root[0]]\n",
    "        else:\n",
    "            if first_root_entry is not None:\n",
    "                return first_root_entry + second_root\n",
    "            else:\n",
    "                return [first_root[0]] + second_root\n",
    "            \n",
    "    ## if it's not a compound        \n",
    "    else:\n",
    "        if first_root_entry is not None:\n",
    "            return first_root_entry\n",
    "        else:\n",
    "            return [first_root[0]]\n",
    "            \n",
    "\n",
    "def root_compounds(word):\n",
    "    if word.startswith(\"'\"):\n",
    "        word = 'a' + word[1:]\n",
    "\n",
    "    # Base case: if word is empty, return empty list\n",
    "    if not word:\n",
    "        return []\n",
    "\n",
    "    roots = []\n",
    "\n",
    "    first_root = dict_word_iterative(word)\n",
    "    if not first_root or not first_root[0]:\n",
    "        # No root found\n",
    "        return []\n",
    "    print(\"first_root\", first_root)\n",
    "    first_root_entry = root_any_word(first_root[0])\n",
    "    print(\"first_root_entry\", first_root_entry)\n",
    "    if first_root_entry is not None:\n",
    "        if isinstance(first_root_entry, list):\n",
    "            roots.extend(first_root_entry)\n",
    "        else:\n",
    "            roots.append(first_root_entry)\n",
    "    else:\n",
    "        roots.append(first_root[0])\n",
    "\n",
    "    # Remove the first root from the word\n",
    "    without_root = word.replace(first_root[0], '', 1)\n",
    "\n",
    "    # If there's nothing left, return the roots found\n",
    "    if not without_root:\n",
    "        return roots\n",
    "\n",
    "    # Check if the word is being reduced\n",
    "    if len(without_root) >= len(word):\n",
    "        # Word not reduced; cannot proceed further\n",
    "        return roots\n",
    "\n",
    "    # Now, recursively process the remaining word\n",
    "    rest_entries = root_any_word(without_root)\n",
    "    if rest_entries is not None:\n",
    "        if isinstance(rest_entries, list):\n",
    "            roots.extend(rest_entries)\n",
    "        else:\n",
    "            roots.append(rest_entries)\n",
    "    else:\n",
    "        # Try dict_word_iterative and ensure result is longer than 2\n",
    "        rest_root = dict_word_iterative(without_root)\n",
    "        if rest_root is None or len(rest_root[0]) < 2:\n",
    "            # Cannot proceed further; return roots found so far\n",
    "            return roots\n",
    "        else:\n",
    "            # Check again if the word is being reduced\n",
    "            if len(without_root) >= len(word):\n",
    "                return roots\n",
    "            # Recursively process the rest of the word\n",
    "            rest_roots = root_compounds(without_root)\n",
    "            roots.extend(rest_roots)\n",
    "\n",
    "    return roots\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inflect(splitted_text):\n",
    "    roots = []\n",
    "    prefixes = ['sva', 'anu', 'sam', 'pra', 'upa', 'vi', 'nis', 'abhi', 'ni', 'pari', 'prati', 'parA', 'ava', 'adhi', 'api', 'ati', 'ud', 'dvi', 'su', 'dur', 'duá¸¥']  # Add more prefixes as needed\n",
    "    i = 0\n",
    "    while i < len(splitted_text):\n",
    "        word = splitted_text[i]\n",
    "        if word in prefixes and i + 1 < len(splitted_text):\n",
    "            next_word = splitted_text[i + 1]\n",
    "            if word == 'sam':\n",
    "                combined_words = ['sam' + next_word, 'saM' + next_word]\n",
    "            elif word == 'vi':\n",
    "                combined_words = ['vi' + next_word, 'vy' + next_word]\n",
    "            else:\n",
    "                combined_words = [word + next_word]\n",
    "\n",
    "            rooted = None\n",
    "            for combined_word in combined_words:\n",
    "                rooted = root_any_word(combined_word)\n",
    "                if rooted is not None:\n",
    "                    break  # Exit loop if a valid root is found\n",
    "\n",
    "            if rooted is not None:\n",
    "                roots.extend(rooted)\n",
    "                i += 2  # Skip next word since it's part of the combined word\n",
    "                continue\n",
    "            else:\n",
    "                rooted_word = root_any_word(word)\n",
    "                if rooted_word is not None:\n",
    "                    roots.extend(rooted_word)\n",
    "                else:\n",
    "                    compound_try = root_compounds(word)\n",
    "                    if compound_try is not None:\n",
    "                        roots.extend(compound_try)\n",
    "                    else:\n",
    "                        roots.append(word)\n",
    "                i += 1  # Move to next word\n",
    "        else:\n",
    "            rooted = root_any_word(word)\n",
    "            if rooted is not None:\n",
    "                roots.extend(rooted)\n",
    "            else:\n",
    "                compound_try = root_compounds(word)\n",
    "                if compound_try is not None:\n",
    "                    roots.extend(compound_try)\n",
    "                else:\n",
    "                    roots.append(word)\n",
    "            i += 1\n",
    "\n",
    "    # Transliterate roots\n",
    "    for j in range(len(roots)):\n",
    "        if isinstance(roots[j], list):\n",
    "            roots[j][0] = transliterateSLP1IAST(roots[j][0].replace('-', ''))\n",
    "        else:\n",
    "            roots[j] = transliterateSLP1IAST(roots[j].replace('-', ''))\n",
    "    return roots\n",
    "\n",
    "\n",
    "## bug with process(\"nÄ«lotpalapatrÄyatÄká¹£Ä«\")    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process(text, *dict_names):\n",
    "\n",
    "\n",
    "    ## if the text is a single word, try to find the word in the dictionary for exact match, the split if it fails\n",
    "\n",
    "    if ' ' not in text:\n",
    "\n",
    "        ## if the text end with a *, remove it and try to find the word in the dictionary for exact match\n",
    "        if text[-1] == '*':\n",
    "            voc_entry = get_voc_entry([anythingToIAST(text[:-1])], *dict_names)\n",
    "            if voc_entry is not None:\n",
    "                return voc_entry\n",
    "            else:\n",
    "                process(text[:-1])\n",
    "\n",
    "        ## if in the text there is a _, use wildcard search directly\n",
    "\n",
    "        if '_' in text or '%' in text:\n",
    "            print(\"wildcard search\", anythingToIAST)\n",
    "            voc_entry = get_voc_entry([anythingToIAST(text)], *dict_names)\n",
    "            if voc_entry is not None:\n",
    "                return voc_entry\n",
    "            else:\n",
    "                process(text)\n",
    "\n",
    "        #print(\"single_word\", text)\n",
    "        transliterated_text = anythingToSLP1(text)     \n",
    "        #print(\"transliterated_text\", transliterated_text)\n",
    "        text = regex.sub('[^\\p{L}\\']', '', transliterated_text)\n",
    "        ## here it should be transliterated to SLP1 before and added aH at the end instead of a\n",
    "        if text[0] == \"'\":\n",
    "            text = 'a' + text[1:]\n",
    "        if text[-1] == 'o':\n",
    "            text = text[:-1] + 'aH'\n",
    "        elif text[-1] == 'S':\n",
    "            text = text[:-1] + 'H'\n",
    "        elif text[-1] == 'y':\n",
    "            text = text[:-1] + 'i'\n",
    "        #print(\"text\", text)\n",
    "        if \"o'\" in text:\n",
    "            modified_text = re.sub(r\"o'\", \"aH a\", text)\n",
    "            #print(\"modified_text\", modified_text)\n",
    "            result = process(modified_text)\n",
    "            return result\n",
    "        result = root_any_word(text)\n",
    "        if result is None and text[-1] == 'M':\n",
    "            text = text[:-1] + 'm'\n",
    "            result = root_any_word(text)\n",
    "        if result is None and text[0:1] == \"ch\":\n",
    "            text = 'S' + text[2:] \n",
    "        if result is not None:\n",
    "            for res in result:\n",
    "                if isinstance(res, list):\n",
    "                    res[0] = transliterateSLP1IAST(res[0].replace('-', ''))\n",
    "                    #print(\"res\", res)\n",
    "            result_vocabulary = get_voc_entry(result, *dict_names)  \n",
    "            print(\"result_vocabulary\", result_vocabulary)\n",
    "            return clean_results(result_vocabulary)\n",
    "        else:\n",
    "            query = [transliterateSLP1IAST(text)]\n",
    "            print(\"query\", query)\n",
    "            result_vocabulary = get_voc_entry(query, *dict_names)  \n",
    "            print(\"result_vocabulary\", result_vocabulary)\n",
    "            if isinstance(result_vocabulary[0][2], dict):\n",
    "            #result_vocabulary[0][0] != result_vocabulary[0][2][0]:\n",
    "                return clean_results(result_vocabulary)\n",
    "    \n",
    "    ## given that the text is composed of multiple words, we split them first then analyse one by one\n",
    "            \n",
    "    ## attempt to remove sandhi and tokenise in any case\n",
    "    #print(\"pre_splitted_text\", text)\n",
    "    text = transliterateSLP1IAST(text)\n",
    "    #print(\"transliterate to split\", text)\n",
    "    splitted_text = sandhi_splitter(text)\n",
    "    splitted_text = [transliterateIASTSLP1(word) for word in splitted_text]    \n",
    "    inflections = inflect(splitted_text) \n",
    "    inflections_vocabulary = get_voc_entry(inflections)\n",
    "    inflections_vocabulary = [entry for entry in inflections_vocabulary if len(entry[0]) > 1]       \n",
    "\n",
    "    return clean_results(inflections_vocabulary)\n",
    "\n",
    "\n",
    "##process(\"dveá¹£ÄnuviddhaÅcetanÄcetanasÄdhanÄdhÄ«nastÄpÄnubhava\")\n",
    "\n",
    "filtered_words = [\"ca\", \"na\", \"eva\", \"ni\", \"apya\", \"ava\", \"sva\"]\n",
    "\n",
    "\n",
    "def clean_results(list_of_entries):\n",
    "\n",
    "    i = 0\n",
    "    while i < len(list_of_entries) - 1:  # Subtract 1 to avoid index out of range error\n",
    "        # Check if the word is in filtered_words\n",
    "        if list_of_entries[i][0] in filtered_words:\n",
    "            while i < len(list_of_entries) - 1 and list_of_entries[i + 1][0] == list_of_entries[i][0]:\n",
    "                del list_of_entries[i + 1]\n",
    "\n",
    "        if len(list_of_entries[i][0]) == 7 and list_of_entries[i][0][-1] == \"n\" and list_of_entries[i][0][4] != list_of_entries[i][0][0]:\n",
    "            replacement = get_voc_entry([list_of_entries[i][0][4]])\n",
    "            print(\"replacement\", replacement)\n",
    "            if replacement is not None:\n",
    "                list_of_entries[i][0] = replacement[0][0]\n",
    "                \n",
    "        \n",
    "        # Check if the word is \"sam\"\n",
    "        if list_of_entries[i][0] == \"sam\":\n",
    "            j = i + 1\n",
    "            while j < len(list_of_entries) and (list_of_entries[j][0] == \"sa\" or list_of_entries[j][0] == \"sam\"):\n",
    "                j += 1\n",
    "            if j < len(list_of_entries):\n",
    "                voc_entry = get_voc_entry([\"sam\" + list_of_entries[j][0]])\n",
    "                print(\"voc_entry\", voc_entry)\n",
    "                if voc_entry[0][0] == voc_entry[0][2][0]:\n",
    "                    print(\"revised query\", [\"saá¹\" + list_of_entries[j][0]])\n",
    "                    voc_entry = get_voc_entry(\"saá¹\" + list_of_entries[j][0])\n",
    "                    print(\"revise_voc_entry\", voc_entry)\n",
    "                if voc_entry is not None:\n",
    "                    list_of_entries[i] = [item for sublist in voc_entry for item in sublist]\n",
    "                    del list_of_entries[i + 1:j + 1]\n",
    "        \n",
    "        # Check if the word is \"anu\"\n",
    "        if list_of_entries[i][0] == \"anu\":\n",
    "            j = i + 1\n",
    "            while j < len(list_of_entries) and (list_of_entries[j][0] == \"anu\"):\n",
    "                j += 1\n",
    "            if j < len(list_of_entries):\n",
    "                voc_entry = get_voc_entry([\"anu\" + list_of_entries[j][0]])\n",
    "                if voc_entry is not None:\n",
    "                    list_of_entries[i] = [item for sublist in voc_entry for item in sublist]\n",
    "                    del list_of_entries[i + 1:j + 1]\n",
    "        \n",
    "        # Check if the word is \"ava\"\n",
    "        if list_of_entries[i][0] == \"ava\":\n",
    "            j = i + 1\n",
    "            while j < len(list_of_entries) and (list_of_entries[j][0] == \"ava\"):\n",
    "                j += 1\n",
    "            if j < len(list_of_entries):\n",
    "                print(\"testing with:\", [\"ava\" + list_of_entries[j + 1][0]])\n",
    "                voc_entry = get_voc_entry([\"ava\" + list_of_entries[j + 1][0]])\n",
    "                if voc_entry is not None:\n",
    "                    list_of_entries[i] = [item for sublist in voc_entry for item in sublist]\n",
    "                    del list_of_entries[i + 1:j + 1]        \n",
    "        i += 1  \n",
    "    return list_of_entries\n",
    "\n",
    "def process_test(text, remove_stopwords = False, dictionary_entry = True, output_encoding = \"IAST\", entry_only = False):\n",
    "    \n",
    "    ## attempt to remove sandhi and tokenise in any case\n",
    "    splitted_text = sandhi_splitter(text)\n",
    "    \n",
    "    ## removes stopwords\n",
    "    if remove_stopwords == True: \n",
    "        splitted_text = remove_stopwords_list(splitted_text)\n",
    "        \n",
    "    \n",
    "    inflections = inflect(splitted_text) \n",
    "    \n",
    "    if dictionary_entry == True:\n",
    "        inflections = get_voc_entry(inflections)    \n",
    "\n",
    "    if entry_only == True:\n",
    "        entry_list = []\n",
    "        for entry in inflections:\n",
    "            entry_list.append(entry[0])\n",
    "        inflections = entry_list    \n",
    "\n",
    "    return clean_results(inflections)\n",
    "\n",
    "## hard mode testing:\n",
    "#process(\"dveá¹£ÄnuviddhaÅcetanÄcetanasÄdhanÄdhÄ«nastÄpÄnubhava\")\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    remove_char_text = ''.join(c for c in text if c.isalpha() or c == \"'\" or c == ' ')\n",
    "\n",
    "    print(\"processing\", text)\n",
    "    ## attempt to remove sandhi and tokenise in any case\n",
    "    splitted_text = sandhi_splitter(remove_char_text)   \n",
    "\n",
    "    splitted_text = remove_stopwords_list(splitted_text)    \n",
    "    \n",
    "    inflections = inflect(splitted_text) \n",
    "    \n",
    "    entry_list = []\n",
    "    entry_list = [entry[0] for entry in inflections if len(entry[0]) > 1]\n",
    "    entry_list = [key for key, group in groupby(entry_list)]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(entry_list) - 1:\n",
    "        # Normalize words to form without diacritics\n",
    "        word1 = unicodedata.normalize('NFD', entry_list[i])\n",
    "        word2 = unicodedata.normalize('NFD', entry_list[i+1])\n",
    "\n",
    "        # Remove diacritics\n",
    "        word1 = ''.join(c for c in word1 if not unicodedata.combining(c))\n",
    "        word2 = ''.join(c for c in word2 if not unicodedata.combining(c))\n",
    "\n",
    "        if word1 in word2 or word2 in word1:\n",
    "            if len(entry_list[i]) > len(entry_list[i+1]):\n",
    "                entry_list.pop(i)\n",
    "            else:\n",
    "                entry_list.pop(i+1)\n",
    "        else:\n",
    "            i += 1\n",
    "    print(\"processed\", entry_list)\n",
    "    return clean_results(entry_list)\n",
    "\n",
    "\n",
    "\n",
    "#dict_names = [\"AE\", \"AP90\", \"MW\", \"MWE\"]\n",
    "\n",
    "#path = \"/resources/Sanskrit_dictionaries/\"\n",
    "def multidict(name: str, *args: str, source: str = \"MW\") -> Dict[str, Dict[str, List[str]]]:\n",
    "    dict_names: List[str] = []\n",
    "    dict_results: Dict[str, Dict[str, List[str]]] = {}\n",
    "    name_component: str = \"\"\n",
    "    \n",
    "    # Collect dictionary names\n",
    "    if not args:\n",
    "        dict_names.append(source)\n",
    "    else:\n",
    "        dict_names.extend(args)\n",
    "    \n",
    "    session = Session()\n",
    "    \n",
    "    # For each dictionary, perform queries and process results\n",
    "    for dict_name in dict_names:\n",
    "        \n",
    "        \n",
    "        # Initial query\n",
    "        query_builder = f\"\"\"\n",
    "        SELECT keys_iast, components, cleaned_body FROM {dict_name + \"clean\"} \n",
    "        WHERE keys_iast = :name \n",
    "        OR keys_iast LIKE :wildcard_name\n",
    "        \"\"\"\n",
    "        wildcard_name = f\"{name}\"\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            results = connection.execute(\n",
    "                text(query_builder), \n",
    "                {\"name\": name, \"wildcard_name\": wildcard_name}\n",
    "            ).fetchall()\n",
    "\n",
    "        # Additional query if no results\n",
    "        if not results and len(name) > 1:\n",
    "            query_builder = f\"\"\"\n",
    "            SELECT keys_iast, components, cleaned_body FROM {dict_name + \"clean\"} \n",
    "            WHERE keys_iast = :name \n",
    "            OR keys_iast LIKE :wildcard_name\n",
    "            \"\"\"\n",
    "            wildcard_name = f\"{name[:-1]}_\"\n",
    "            \n",
    "            with engine.connect() as connection:\n",
    "                results = connection.execute(\n",
    "                    text(query_builder), \n",
    "                    {\"name\": name, \"wildcard_name\": wildcard_name}\n",
    "                ).fetchall()\n",
    "        \n",
    "        #print(f\"Results for {dict_name}: {results}\")\n",
    "        \n",
    "        # Additional query if no results\n",
    "        if not results and len(name) > 1:\n",
    "            query_builder = f\"\"\"\n",
    "            SELECT keys_iast, components, cleaned_body FROM {dict_name + \"clean\"} \n",
    "            WHERE keys_iast LIKE :name1 \n",
    "            OR keys_iast LIKE :name2\n",
    "            \"\"\"\n",
    "            with engine.connect() as connection:\n",
    "                results = connection.execute(\n",
    "                    text(query_builder), \n",
    "                    {\"name1\": name + \"_\", \"name2\": name[:-1] + \"_\"}\n",
    "                ).fetchall()\n",
    "\n",
    "        #print(f\"Results for {dict_name} after second query: {results}\")\n",
    "        \n",
    "\n",
    "        # Group results by components\n",
    "        component_dict: Dict[str, List[str]] = {}\n",
    "        for row in results:\n",
    "            #print(f\"Row: {row}\")\n",
    "            key_iast, components, cleaned_body = row\n",
    "            if not name_component:\n",
    "                name_component = components\n",
    "            #print(f\"key_iast: {key_iast}, components: {components}, cleaned_body: {cleaned_body}\")\n",
    "            if key_iast in component_dict:\n",
    "                component_dict[key_iast].append(cleaned_body)\n",
    "            else:\n",
    "                component_dict[key_iast] = [cleaned_body]        \n",
    "        # Add to dict_results\n",
    "        dict_results[dict_name] = component_dict\n",
    "    \n",
    "    connection.close()\n",
    "    return [name_component, dict_results]\n",
    "\n",
    "# Example usage\n",
    "#results = multidict(\"yoga\", \"MW\" \"AP90\")\n",
    "#print(results)\n",
    "\n",
    "\n",
    "\n",
    "def get_voc_entry(list_of_entries, *args: str, source: str = \"MW\"):\n",
    "    entries = []\n",
    "    for entry in list_of_entries:        \n",
    "        if isinstance(entry, list):\n",
    "            \n",
    "            word = entry[0]\n",
    "            dict_entry = []\n",
    "            if word in mwdictionaryKeys:  # Check if the key exists ## check non nel dizionario, ma solo nella lista chiavi\n",
    "                key2 = dictionary_json[word][0][1] ##qui si dovrebbe fare un fetch SQL \n",
    "                key2 = transliterateSLP1IAST(key2) ## qui ho un leggero problema, se prendo l'equivalente in SQL da dove prendo la key 2? dalla prima o dalla seconda voce. Possibilmente dalla seconda se c'Ã¨ piÃ¹ di una voce. \n",
    "    \n",
    "                for entry_dict in dictionary_json[word]:\n",
    "                    dict_entry.append(re.sub(r'<s>(.*?)</s>', lambda m: '<s>' + transliterateSLP1IAST(m.group(1)) + '</s>', entry_dict[4]))\n",
    "                entry.append(key2)\n",
    "                entry.append(dict_entry)\n",
    "            else:\n",
    "                entry.append(word)  # Append the original word for key2\n",
    "                entry.append([word])  # Append the original word for dict_entry\n",
    "            entries.append(entry)\n",
    "            \n",
    "        elif isinstance(entry, str):\n",
    "            \n",
    "            dict_entry = []\n",
    "            if entry in mwdictionaryKeys:  # Check if the key exists\n",
    "                key2 = dictionary_json[entry][0][1]\n",
    "                key2 = transliterateSLP1IAST(key2)\n",
    "    \n",
    "                for entry_dict in dictionary_json[entry]:\n",
    "                    dict_entry.append(re.sub(r'<s>(.*?)</s>', lambda m: '<s>' + transliterateSLP1IAST(m.group(1)) + '</s>', entry_dict[4]))\n",
    "                entry = [entry]    \n",
    "                \n",
    "                entry.append(key2)\n",
    "                entry.append(dict_entry)\n",
    "            else:\n",
    "                entry = [entry, entry, [entry]]  # Append the original word for key2 and dict_entry\n",
    "            entries.append(entry)\n",
    "    return entries\n",
    "\n",
    "# find_inflection = False, inflection_table = False, \n",
    "#split_compounds = True, dictionary_search = False,\n",
    "##first attempt to process the word not using the sandhi_splitter, which often gives uncorrect;\n",
    "##then if the word is not found, try to split the word in its components and find the root of each component\n",
    "\n",
    "\n",
    "def get_mwword(word:str)->list[str, list[str]] : \n",
    "        session = Session()\n",
    "        query_builder = \"SELECT key2, cleaned_body FROM mwclean WHERE keys_iast = :word\"\n",
    "        results = session.execute(query_builder, {'word': word}).fetchall()\n",
    "        session.close()        \n",
    "        components = results[0][0]\n",
    "        result_list = [row[1] for row in results]\n",
    "        \n",
    "        return [components, result_list]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_voc_entry(list_of_entries):\n",
    "    entries = []\n",
    "    for entry in list_of_entries:        \n",
    "        if isinstance(entry, list):\n",
    "\n",
    "            word = entry[0]\n",
    "\n",
    "            if word in mwdictionaryKeys:  # Check if the key exists ## check non nel dizionario, ma solo nella lista chiavi\n",
    "                entry = entry + get_mwword(word)\n",
    "            else:\n",
    "                entry = [entry, entry, [entry]]  # Append the original word for key2 and dict_entry\n",
    "            entries.append(entry)\n",
    "            \n",
    "        elif isinstance(entry, str):\n",
    "            \n",
    "            if entry in mwdictionaryKeys:  # Check if the key exists\n",
    "                entry = [entry] + get_mwword(entry)\n",
    "            else:\n",
    "                entry = [entry, entry, [entry]]  # Append the original word for key2 and dict_entry\n",
    "            entries.append(entry)\n",
    "    return entries\n",
    "\n",
    "# find_inflection = False, inflection_table = False, \n",
    "#split_compounds = True, dictionary_search = False,\n",
    "##first attempt to process the word not using the sandhi_splitter, which often gives uncorrect;\n",
    "##then if the word is not found, try to split the word in its components and find the root of each component\n",
    "\n",
    "\n",
    "\n",
    "def get_voc_entry(list_of_entries, *dict_names):\n",
    "    entries = []\n",
    "    for entry in list_of_entries:        \n",
    "        if isinstance(entry, list):\n",
    "\n",
    "            word = entry[0]\n",
    "\n",
    "            if '*' not in word and '_' not in word and '%' not in word:\n",
    "                if word in mwdictionaryKeys:  # Check if the key exists ## check non nel dizionario, ma solo nella lista chiavi\n",
    "                    print(\"word\", word)\n",
    "                    entry = entry + multidict(word, *dict_names)\n",
    "                else:\n",
    "                    entry = [entry, entry, [entry]]  # Append the original word for key2 and dict_entry\n",
    "                entries.append(entry)\n",
    "            else:\n",
    "                print(\"wildcard search\", word)\n",
    "                entry = entry + multidict(word, *dict_names)\n",
    "                entries.append(entry)\n",
    "            \n",
    "        elif isinstance(entry, str):\n",
    "            \n",
    "            if '*' not in entry and '_' not in entry and '%' not in entry:\n",
    "                if entry in mwdictionaryKeys:  # Check if the key exists ## check non nel dizionario, ma solo nella lista chiavi\n",
    "                    print(\"word\", entry)\n",
    "                    entry = [entry] + multidict(entry, *dict_names)\n",
    "                else:\n",
    "                    entry = [entry, entry, [entry]]  # Append the original word for key2 and dict_entry\n",
    "                entries.append(entry)\n",
    "            else:\n",
    "                print(\"wildcard search\", entry)\n",
    "                entry = [entry] + multidict(entry, *dict_names)\n",
    "                entries.append(entry)\n",
    "    return entries\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
